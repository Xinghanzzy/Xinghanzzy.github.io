<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>机器翻译工具、trick</title>
      <link href="/2019/05/07/t/"/>
      <url>/2019/05/07/t/</url>
      
        <content type="html"><![CDATA[<ul><li>分词工具 sentence piece</li><li>数据选择 Xenc</li><li>对齐工具 fastalign</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>机器翻译工具、trick</title>
      <link href="/2019/05/07/trick/"/>
      <url>/2019/05/07/trick/</url>
      
        <content type="html"><![CDATA[<ul><li>分词工具 sentence piece</li><li>数据选择 Xenc</li><li>对齐工具 fastalign</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数学概念：张量、矩阵、叉乘、点乘</title>
      <link href="/2019/05/06/tensor-dot-cross-product/"/>
      <url>/2019/05/06/tensor-dot-cross-product/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://www.jianshu.com/p/abe7515c6c7f" target="_blank" rel="noopener">https://www.jianshu.com/p/abe7515c6c7f</a> </p></blockquote><h1 id="乘法"><a href="#乘法" class="headerlink" title="乘法"></a>乘法</h1><h2 id="点乘-dot-product"><a href="#点乘-dot-product" class="headerlink" title="点乘(dot product)"></a>点乘(dot product)</h2><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><p>在数学中，数量积（dot product; scalar product，也称为点积）是接受在实数R上的两个向量并返回一个实数值标量的二元运算。它是欧几里得空间的标准内积。</p><h3 id="理解"><a href="#理解" class="headerlink" title="理解"></a>理解</h3><p><strong>就是两个向量对应位置一一相乘后求和的操作，最后结果是一个标量，是一个实数值。</strong></p><p>两个向量a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：</p><p>a·b=a1b1+a2b2+……+anbn。</p><p>使用矩阵乘法并把（纵列）向量当作n×1 矩阵，点积还可以写为：</p><p>a·b=a^T*b，这里的a^T指示矩阵a的转置。</p><h2 id="叉乘-cross-product"><a href="#叉乘-cross-product" class="headerlink" title="叉乘(cross product)"></a>叉乘(cross product)</h2><h3 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h3><p>向量积，数学中又称外积、叉积，物理中称矢积、叉乘，是一种在向量空间中向量的二元运算。与点积不同，它的运算结果是一个向量而不是一个标量。并且两个向量的叉积与这两个向量和垂直。其应用也十分广泛，通常应用于物理学光学和计算机图形学中。</p><h3 id="理解-1"><a href="#理解-1" class="headerlink" title="理解"></a>理解</h3><p>这个是两个向量的乘法，得到的是另一个向量</p><h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><h3 id="定义-2"><a href="#定义-2" class="headerlink" title="定义"></a>定义</h3><p>矩阵相乘最重要的方法是一般矩阵乘积。它只有在第一个矩阵的列数（column）和第二个矩阵的行数（row）相同时才有意义 [1]  。一般单指矩阵乘积时，指的便是一般矩阵乘积。一个m×n的矩阵就是m×n个数排成m行n列的一个数阵。由于它把许多数据紧凑的集中到了一起，所以有时候可以简便地表示一些复杂的模型。</p><h3 id="理解-2"><a href="#理解-2" class="headerlink" title="理解"></a>理解</h3><p>深度学习中常用</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.matmul()</span><br><span class="line"><span class="comment"># 3-D tensor `a`</span></span><br><span class="line">a = tf.constant(np.arange(<span class="number">1</span>, <span class="number">13</span>, dtype=np.int32),</span><br><span class="line">                shape=[2, 2, 3])                  =&gt; [[[ 1.  2.  3.]</span><br><span class="line">                                                       [ <span class="number">4.</span>  <span class="number">5.</span>  <span class="number">6.</span>]],</span><br><span class="line">                                                      [[ <span class="number">7.</span>  <span class="number">8.</span>  <span class="number">9.</span>]</span><br><span class="line">                                                       [<span class="number">10.</span> <span class="number">11.</span> <span class="number">12.</span>]]]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3-D tensor `b`</span></span><br><span class="line">b = tf.constant(np.arange(<span class="number">13</span>, <span class="number">25</span>, dtype=np.int32),</span><br><span class="line">                shape=[2, 3, 2])                   =&gt; [[[13. 14.]</span><br><span class="line">                                                        [<span class="number">15.</span> <span class="number">16.</span>]</span><br><span class="line">                                                        [<span class="number">17.</span> <span class="number">18.</span>]],</span><br><span class="line">                                                       [[<span class="number">19.</span> <span class="number">20.</span>]</span><br><span class="line">                                                        [<span class="number">21.</span> <span class="number">22.</span>]</span><br><span class="line">                                                        [<span class="number">23.</span> <span class="number">24.</span>]]]</span><br><span class="line">c = tf.matmul(a, b) =&gt; [[[ 94 100]</span><br><span class="line">                         [<span class="number">229</span> <span class="number">244</span>]],</span><br><span class="line">                        [[<span class="number">508</span> <span class="number">532</span>]</span><br><span class="line">                         [<span class="number">697</span> <span class="number">730</span>]]]</span><br></pre></td></tr></table></figure><h1 id="标量、向量、矩阵、张量"><a href="#标量、向量、矩阵、张量" class="headerlink" title="标量、向量、矩阵、张量"></a>标量、向量、矩阵、张量</h1><h2 id="标量"><a href="#标量" class="headerlink" title="标量"></a>标量</h2><p>一个标量就是一个单独的数，一般用小写的的变量名称表示。</p><p>一个实数值</p><h2 id="向量"><a href="#向量" class="headerlink" title="向量"></a>向量</h2><p>一个向量就是一列数，这些数是有序排列的。用过次序中的索引，我们可以确定每个单独的数。通常会赋予向量粗体的小写名称。当我们需要明确表示向量中的元素时，我们会将元素排列成一个方括号包围的纵柱：</p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g2rnm97tiwj203q069t8s.jpg" alt></p><p>我们可以把向量看作空间中的点，每个元素是不同的坐标轴上的坐标。</p><h2 id="矩阵"><a href="#矩阵" class="headerlink" title="矩阵"></a>矩阵</h2><p>矩阵是二维数组，其中的每一个元素被两个索引而非一个所确定。我们通常会赋予矩阵粗体的大写变量名称，比如A。 </p><p>如果我们现在有N个用户的数据，每条数据含有M个特征，那其实它对应的就是一个N*M的矩阵</p><p>如果一张图由16*16的像素点组成，那这就是一个16*16的矩阵</p><h2 id="张量"><a href="#张量" class="headerlink" title="张量"></a>张量</h2><p>几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，那么矩阵就是二阶张量。 </p><p>可以将任意一张彩色图片表示成一个三阶张量，三个维度分别是图片的高度、宽度和色彩数据。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 数学 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知乎人生格言</title>
      <link href="/2019/05/04/zhihu-motto/"/>
      <url>/2019/05/04/zhihu-motto/</url>
      
        <content type="html"><![CDATA[<ul><li><p>《孟子・离娄上》<em>:“孟子曰:人之患在<strong>好为人师</strong>。</em>”  </p></li><li><p>交朋友的标准是什么？</p><p>出世的智者</p><p>入世的强者</p><p>或者正常而阳光的普通人</p></li><li><p>别让孩子输在起跑线上 有道理吗？</p><p>一辈子都要和别人去比较</p><p>是人生悲剧的源头</p></li><li><p>做哪些事情可以提升生活品质？</p><p>定期扔东西</p></li><li><p>结婚</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 知乎 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>wmt19</title>
      <link href="/2019/04/23/wmt19/"/>
      <url>/2019/04/23/wmt19/</url>
      
        <content type="html"><![CDATA[<h1 id="WMT有效方法"><a href="#WMT有效方法" class="headerlink" title="WMT有效方法"></a>WMT有效方法</h1><h2 id="Back-Translation"><a href="#Back-Translation" class="headerlink" title="Back-Translation"></a>Back-Translation</h2><h4 id="zh2en"><a href="#zh2en" class="headerlink" title="zh2en"></a>zh2en</h4><p>back-translation with beam search </p><h4 id="de2en"><a href="#de2en" class="headerlink" title="de2en"></a>de2en</h4><p>back-translation with sampling </p><h2 id="DLV"><a href="#DLV" class="headerlink" title="DLV"></a>DLV</h2><p>30层深层</p><h2 id="Distillation-by-ensemble-teacher"><a href="#Distillation-by-ensemble-teacher" class="headerlink" title="Distillation by ensemble teacher"></a>Distillation by ensemble teacher</h2><p>用DLV 8model的ensemble结果做数据然后在8个模型上finetuning</p><p>之后再挑选模型ensemble</p><h2 id="Hypothesis-combination"><a href="#Hypothesis-combination" class="headerlink" title="Hypothesis combination"></a>Hypothesis combination</h2><p>多个ensemble模型的nbest拿出来做reranking</p>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Back-Translation和源语端数据增强</title>
      <link href="/2019/04/22/Backtranslation-and-source/"/>
      <url>/2019/04/22/Backtranslation-and-source/</url>
      
        <content type="html"><![CDATA[<h1 id="Back-Translation"><a href="#Back-Translation" class="headerlink" title="Back-Translation"></a>Back-Translation</h1><h1 id="源语端数据增强"><a href="#源语端数据增强" class="headerlink" title="源语端数据增强"></a>源语端数据增强</h1><p>这是自动化所张家俊老师2016年的ACL文章《Exploiting Source-side Monolingual Data in Neural Machine Translation》</p><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>我们两种方法来充分利用NMT中的源边单语数据。 </p><ol><li>第一种方法采用自学习算法生成用于NMT训练的合成大规模并行数据。 </li><li>第二种方法应用多任务学习框架，使用两个NMT同时预测翻译和重新排序的源侧单语句。</li></ol><p>都是利用单语数据<strong>提高Encoder</strong>性能的方法</p><p>选用<strong>50%</strong>的单词在词表中的源语单语数据比较好（论文中未指出数据量的影响）</p><h2 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h2><h3 id="自学习算法"><a href="#自学习算法" class="headerlink" title="自学习算法"></a>自学习算法</h3><p>使用可用的对齐句子对构建基线机器翻译系统，然后通过将源侧单语句与基线系统进行翻译来获得更多的合成并行数据。</p><p>合成目标部分可能对NMT的解码器模型产生负面影响。为了解决这个问题，我们可以通过<strong>冻结</strong>合成数据的<strong>解码器网络参数</strong>来区分NMT训练期间合成双语句子的原始双字。</p><p>自学习算法可以改进NMT的Encoder。</p><p><strong>有效性推测：</strong></p><p>​    Encoder见到更多的情况，提取信息能力变强</p><blockquote><p>源侧单语数据在词汇表中提供了更多的单词排列。 我们的RNN编码器网络模型将进行优化，以便很好地解释所有单词排列。 </p><p>the source-side monolingual data provides much more permutations of words in the vocabulary. Our RNN encoder network model will be optimized to well explain all of the word permutations. </p></blockquote><h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>应用多任务学习框架来同时预测目标翻译和重新排序的源侧句子。背后的主要思想是我们构建了两个NMT：一个是在对齐的句子对上训练来预测来自源句子的目标句子，而另一个是在源侧单语语料库上训练来预测来自原始来源的重新排序的源句子sentences1。应当注意，两个NMT共享相同的编码器网络，以便它们可以相互帮助以加强编码器模型。</p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g2bb0yhdavj20fi0eh0tk.jpg" alt></p><ul><li><p>machine translation task</p><p>input：src</p><p>lable：tgt</p></li><li><p>sentence reordering task</p><p>input：src</p><p>lable：src_reorder</p><p>pre-ordering rules proposed by (Wang et al., 2007),</p><p>一个将src按照tgt语言语序排列的工具(很慢)</p><blockquote><p>which can permutate the words of the source sentence so as to approximate the target language word order.</p><p>它可以排列源句的单词，以便近似目标语言单词顺序。</p></blockquote></li></ul><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g2bby7r8bbj20fg05h0t2.jpg" alt></p><ul><li><p>训练</p><p><strong>1</strong>epoch sentence reordering task + <strong>4</strong> epoch machine translation task</p></li></ul><h2 id="结果"><a href="#结果" class="headerlink" title="结果"></a>结果</h2><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g2bc288yyaj20wu0gbdk5.jpg" alt></p>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Train in Deep Learning</title>
      <link href="/2019/04/18/Train-DeepLearning/"/>
      <url>/2019/04/18/Train-DeepLearning/</url>
      
        <content type="html"><![CDATA[<blockquote><p>自己看的记录文档</p></blockquote><h3 id="epoch、steps"><a href="#epoch、steps" class="headerlink" title="epoch、steps"></a>epoch、steps</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">total = total * epoch</span><br><span class="line">train_steps = total*<span class="number">5</span>/(batchsize*<span class="number">4</span>)/workergpu</span><br></pre></td></tr></table></figure><h3 id="finetuning"><a href="#finetuning" class="headerlink" title="finetuning"></a>finetuning</h3><ul><li>finetuning是使用一样的词表进行训练</li><li>模型(output dir)里面放置预训练好的模型</li><li>finetuning的train_steps是在原模型的train_steps上进行增加</li></ul><h3 id="loss-in-T2T-and-Fairseq"><a href="#loss-in-T2T-and-Fairseq" class="headerlink" title="loss in T2T and Fairseq"></a>loss in T2T and Fairseq</h3><p>T2T中的Loss是考虑之前所有epoch(batch?)的训练，然后经过平滑得出来的Loss</p><p>Fairseq则不同</p><blockquote><p>Todo: 计算细节</p></blockquote><p><strong>经验：</strong></p><ul><li>loss(ppl)不再下降：训练差不多了</li><li>Fairseq中loss可能最终掉到  valid_loss 4.08244 | valid_nll_loss 2.36351 这个数值</li><li>T2T中的loss可能最终是不到2分</li></ul><h3 id="train-sh"><a href="#train-sh" class="headerlink" title="train.sh"></a>train.sh</h3><p>一个写的不错的训练脚本</p><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span>! /usr/bin/bash</span><br><span class="line">set -e</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>####### hardware ########</span><br><span class="line"><span class="meta">#</span> devices</span><br><span class="line">dev=0,1,2,3,4,5,6,7</span><br><span class="line"><span class="meta">#</span> how many percentages per gpu</span><br><span class="line">gpu_fraction=0.95</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>####### dataset ########</span><br><span class="line"><span class="meta">#</span> language: zh2en or en2zh</span><br><span class="line"><span class="meta">#</span>lang=de2en</span><br><span class="line">lang=en2de</span><br><span class="line"><span class="meta">#</span> datatype= version + segmentation</span><br><span class="line">datatype=v1-bpe40k</span><br><span class="line"><span class="meta">#</span> dataset: cwmt/wmt</span><br><span class="line">dataset=wmt14</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>####### parameters ########</span><br><span class="line"><span class="meta">#</span> which model</span><br><span class="line">model=transformer</span><br><span class="line"><span class="meta">#</span>model=transformer_dla</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> which hparams</span><br><span class="line">param=transformer_base</span><br><span class="line"><span class="meta">#</span>param=transformer_base_v2</span><br><span class="line"><span class="meta">#</span>param=transformer_base_v3</span><br><span class="line"><span class="meta">#</span>param=transformer_before</span><br><span class="line"><span class="meta">#</span>param=transformer_before_shared25</span><br><span class="line"><span class="meta">#</span>param=transformer_big_multistep2</span><br><span class="line"><span class="meta">#</span>param=transformer_dla_base</span><br><span class="line"><span class="meta">#</span>param=transformer_dla_base25_shared</span><br><span class="line"><span class="meta">#</span>param=transformer_dla_base30_shared</span><br><span class="line"><span class="meta">#</span>param=transformer_dla_base20_shared_filter4096</span><br><span class="line"><span class="meta">#</span>param=transformer_dla_rpr_base25</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>####### required ########</span><br><span class="line"><span class="meta">#</span> tag is the name of your experiments, must exist</span><br><span class="line"><span class="meta">#</span>tag=sampling_r2l_v3_20m</span><br><span class="line"><span class="meta">#</span>tag=sampling_r2l_dla20_v3_20m</span><br><span class="line"><span class="meta">#</span>tag=base_r2l_dla_rpr25_20m</span><br><span class="line"><span class="meta">#</span>tag=base_dla30_seed30_20m</span><br><span class="line"><span class="meta">#</span>tag=base_before_shared25_seed2_20m</span><br><span class="line"><span class="meta">#</span>tag=base_dla25_20m</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>tag=finetune_distillation_v3_10m</span><br><span class="line"><span class="meta">#</span>tag=finetune_distillation_dla30_10m</span><br><span class="line"><span class="meta">#</span>tag=finetune_distillation_dla25_10m</span><br><span class="line"><span class="meta">#</span>tag=finetune_distillation_dla_rpr25_2_10m</span><br><span class="line"><span class="meta">#</span>tag=finetune_distillation_dla20_v3_2_10m</span><br><span class="line">tag=base_textRepair</span><br><span class="line">random_seed=2</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>if [ $lang == "de2en" ]; then</span><br><span class="line"><span class="meta">#</span># for CWMT zh-en task, training data contains CWMT + 30%WMT, about 1000w, epoch=15, need 130k steps</span><br><span class="line"><span class="meta">#</span>        # 39207</span><br><span class="line"><span class="meta">#</span>if [ $dataset == "wmt" ]; then</span><br><span class="line"><span class="meta">#</span>train_step=100000</span><br><span class="line"><span class="meta">#</span>else</span><br><span class="line"><span class="meta">#</span>echo "unknow dataset:$dataset"</span><br><span class="line"><span class="meta">#</span>exit</span><br><span class="line"><span class="meta">#</span>fi</span><br><span class="line"><span class="meta">#</span>elif [ $lang == "en2de" ]; then</span><br><span class="line"><span class="meta">#</span>        # for CWMT or WMT en-zh task, training data contains CWMT + 30%WMT + xinhua, about 1693w, epoch=15, need 271k steps</span><br><span class="line"><span class="meta">#</span>        if [ $dataset == "wmt" ]; then</span><br><span class="line"><span class="meta">#</span>                train_step=100000</span><br><span class="line"><span class="meta">#</span>        else</span><br><span class="line"><span class="meta">#</span>                echo "unknow dataset:$dataset"</span><br><span class="line"><span class="meta">#</span>                exit</span><br><span class="line"><span class="meta">#</span>        fi</span><br><span class="line"><span class="meta">#</span>fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> dynamic hparams, e.g. change the batch size without the register in code, other_hparams='batch_size=2048'</span><br><span class="line"><span class="meta">#</span>other_hparams='batch_size=2048'</span><br><span class="line">other_hparams=</span><br><span class="line"></span><br><span class="line">train_step=50000</span><br><span class="line"><span class="meta">#</span> automatically set worker_gpu according to $dev</span><br><span class="line">worker_gpu=`echo "$dev" | awk '&#123;split($0,arr,",");print length(arr)&#125;'`</span><br><span class="line"><span class="meta">#</span> dir of training data</span><br><span class="line">data_dir=./data/$lang/$datatype-$dataset/data-bin</span><br><span class="line"><span class="meta">#</span> dir of models</span><br><span class="line">output_dir=./output/$lang/$datatype-$dataset/$tag</span><br><span class="line">if [ ! -d "$output_dir" ]; then</span><br><span class="line">  mkdir -p $output_dir</span><br><span class="line">fi</span><br><span class="line"><span class="meta">#</span> save train.sh</span><br><span class="line">cp `pwd`/train_fairseq.sh $output_dir</span><br><span class="line"></span><br><span class="line">cmd="python3 ../tensor2tensor/bin/t2t-trainer</span><br><span class="line">--problem=translate_ende_wmt32k </span><br><span class="line">--model=$model </span><br><span class="line">--hparams_set=$param </span><br><span class="line">--data_dir=$data_dir </span><br><span class="line">--output_dir=$output_dir </span><br><span class="line">--worker_gpu_memory_fraction=$gpu_fraction</span><br><span class="line">--train_steps=$train_step</span><br><span class="line">--random_seed=$random_seed </span><br><span class="line">--worker_gpu=$worker_gpu</span><br><span class="line">--local_eval_frequency=0"</span><br><span class="line">if [ -n "$other_hparams" ]; then</span><br><span class="line">cmd=$&#123;cmd&#125;" --hparams="$&#123;other_hparams&#125;</span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>echo "run command:"$cmd</span><br><span class="line"><span class="meta">#</span> start training</span><br><span class="line"><span class="meta">#</span>CUDA_VISIBLE_DEVICES=$dev PYTHONPATH=`pwd`/.. nohup $cmd exec 1&gt; $output_dir/train.log exec 2&gt;&amp;1 &amp;</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> start multi-gpu evaluation on CPU</span><br><span class="line">cmd="python3 -u fairseq/train.py </span><br><span class="line"><span class="meta">$</span>data_dir </span><br><span class="line">-a transformer --optimizer adam </span><br><span class="line">--lr 0.0007 </span><br><span class="line">-s en -t de --label-smoothing 0.1 </span><br><span class="line">--dropout 0.1 --max-tokens 4096 </span><br><span class="line">--min-lr 1e-09 --lr-scheduler inverse_sqrt </span><br><span class="line">--weight-decay 0.0001 </span><br><span class="line">--criterion label_smoothed_cross_entropy </span><br><span class="line">--max-update 100000 </span><br><span class="line">--update-freq 1 --warmup-updates 4000 </span><br><span class="line">--warmup-init-lr 1e-07 </span><br><span class="line">--save-dir $output_dir</span><br><span class="line">"</span><br><span class="line">adam_betas="'(0.9,0.98)'"</span><br><span class="line">cmd=$&#123;cmd&#125;" --adam-betas "$&#123;adam_betas&#125;</span><br><span class="line">echo "run command:"$cmd</span><br><span class="line"><span class="meta">#</span>CUDA_VISIBLE_DEVICES=$dev PYTHONPATH=`pwd`/.. nohup $cmd  $output_dir/train.log 2&gt;&amp;1 &amp;</span><br><span class="line">cmd="CUDA_VISIBLE_DEVICES=$dev PYTHONPATH=`pwd`/. nohup "$&#123;cmd&#125;" &gt; $output_dir/train.log 2&gt;&amp;1 &amp;"</span><br><span class="line">eval $cmd</span><br><span class="line">echo $cmd</span><br><span class="line">tail -f $output_dir/train.log</span><br><span class="line"></span><br><span class="line">cmd="python3 ../tensor2tensor/bin/t2t-eval </span><br><span class="line">  --data_dir=$data_dir</span><br><span class="line">  --problems=wmt_zhen_tokens_32k </span><br><span class="line">  --model=$model </span><br><span class="line">  --hparams_set=$param </span><br><span class="line">  --local_eval_frequency=0 </span><br><span class="line">  --densenet=False </span><br><span class="line">  --eval_step=8 </span><br><span class="line">  --locally_shard_to_cpu=False </span><br><span class="line">  --output_dir=$output_dir</span><br><span class="line">  --train_steps=$train_step"</span><br><span class="line"><span class="meta">#</span>CUDA_VISIBLE_DEVICES=-1 PYTHONPATH=`pwd`/.. nohup $cmd exec 1&gt; $output_dir/eval.log exec 2&gt;&amp;1 &amp; </span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> monitor training log</span><br><span class="line">tail -f $output_dir/train.log</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>num translation</title>
      <link href="/2019/04/17/num-translation/"/>
      <url>/2019/04/17/num-translation/</url>
      
        <content type="html"><![CDATA[<blockquote><p>施工中</p></blockquote><h2 id="对齐关系"><a href="#对齐关系" class="headerlink" title="对齐关系"></a>对齐关系</h2><p>对齐关系的获取是这里面的难点</p><p>我们的翻译模型用的是谷歌的Transformer模型</p><blockquote><p> tensor2tensor==1.0.14 </p><p>tensorflow==1.4.0</p><p>CUDA 8.0 Python3</p></blockquote><p>Transformer模型最大的特点是对注意力机制$Attention$的大量使用</p><p>而$Encoder-Decoder Attention$是用$Encoder$层的输出以及$Decoder$上一层的输出作为输入进行计算</p><p>即：</p><script type="math/tex; mode=display">Query:Encoder\_Output</script><script type="math/tex; mode=display">Key=Value:Decoder(上一层输出)</script><p>在这个计算中，Q K相乘获得的矩阵隐性包含着我们需要的对齐信息。</p><p>Q K 相乘的矩阵是 （h*batch_size,len_q,len_k）的</p><p>我们对最后一层的QK矩阵多头取平均，就得到了我想要的结果</p>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FastText</title>
      <link href="/2019/04/15/FastText/"/>
      <url>/2019/04/15/FastText/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/john_bh/article/details/79268850" target="_blank" rel="noopener">https://blog.csdn.net/john_bh/article/details/79268850</a> </p></blockquote><h1 id="FastText：快速的文本分类器"><a href="#FastText：快速的文本分类器" class="headerlink" title="FastText：快速的文本分类器"></a>FastText：快速的文本分类器</h1><h1 id="一、简介"><a href="#一、简介" class="headerlink" title="一、简介"></a>一、简介</h1><p>fasttext是facebook开源的一个词向量与文本分类工具，在2016年开源，典型应用场景是“带监督的文本分类问题”。提供简单而高效的文本分类和表征学习的方法，性能比肩深度学习而且速度更快。</p><p>FastText结合了自然语言处理和机器学习中最成功的理念。这些包括了使用词袋以及n-gram袋表征语句，还有使用子字(subword)信息，并通过隐藏表征在类别间共享信息。我们另外采用了一个softmax层级(利用了类别不均衡分布的优势)来加速运算过程。</p><p>这些不同概念被用于两个不同任务：</p><ul><li><strong>有效文本分类</strong> ：有监督学习</li><li><strong>学习词向量表征</strong>：无监督学习</li></ul><p>举例来说：fastText能够学会“男孩”、“女孩”、“男人”、“女人”指代的是特定的性别，并且能够将这些数值存在相关文档中。然后，当某个程序在提出一个用户请求（假设是“我女友现在在儿？”），它能够马上在fastText生成的文档中进行查找并且理解用户想要问的是有关女性的问题。</p><h1 id="二、FastText原理"><a href="#二、FastText原理" class="headerlink" title="二、FastText原理"></a>二、FastText原理</h1><p>fastText方法包含三部分，<strong>模型架构，层次SoftMax和N-gram特征。</strong></p><h2 id="2-1-模型架构"><a href="#2-1-模型架构" class="headerlink" title="2.1 模型架构"></a>2.1 模型架构</h2><p>fastText的架构和word2vec中的CBOW的架构类似，因为它们的作者都是Facebook的科学家Tomas Mikolov，而且确实fastText也算是words2vec所衍生出来的。</p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g238wxf4w4j20b005l0t1.jpg" alt></p><p>fastText 模型输入一个词的序列（一段文本或者一句话)，输出这个词序列属于不同类别的概率。 </p><p>序列中的词和词组组成特征向量，特征向量通过线性变换映射到中间层，中间层再映射到标签。 </p><p>fastText 在预测标签时使用了非线性激活函数，但在中间层不使用非线性激活函数。</p><p>fastText 模型架构和 Word2Vec 中的 CBOW 模型很类似。不同之处在于，fastText 预测标签，而 CBOW 模型预测中间词。</p><h2 id="2-2-层次SoftMax"><a href="#2-2-层次SoftMax" class="headerlink" title="2.2 层次SoftMax"></a>2.2 层次SoftMax</h2><p>对于有大量类别的数据集，fastText使用了一个分层分类器（而非扁平式架构）。不同的类别被整合进树形结构中（想象下二叉树而非 list）。在某些文本分类任务中类别很多，计算线性分类器的复杂度高。为了改善运行时间，fastText 模型使用了层次 Softmax 技巧。层次 Softmax 技巧建立在哈弗曼编码的基础上，对标签进行编码，能够极大地缩小模型预测目标的数量。</p><p>fastText 也利用了类别（class）不均衡这个事实（一些类别出现次数比其他的更多），通过使用 Huffman 算法建立用于表征类别的树形结构。因此，频繁出现类别的树形结构的深度要比不频繁出现类别的树形结构的深度要小，这也使得进一步的计算效率更高。 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g239noqvtgj20iz0c0tct.jpg" alt></p><h2 id="2-3-N-gram特征"><a href="#2-3-N-gram特征" class="headerlink" title="2.3 N-gram特征"></a>2.3 N-gram特征</h2><p>FastText 可以用于文本分类和句子分类。不管是文本分类还是句子分类，我们常用的特征是词袋模型。但词袋模型不能考虑词之间的顺序，因此 fastText 还加入了 N-gram 特征。“我 爱 她” 这句话中的词袋模型特征是 “我”，“爱”, “她”。这些特征和句子 “她 爱 我” 的特征是一样的。如果加入 2-Ngram，第一句话的特征还有 “我-爱” 和 “爱-她”，这两句话 “我 爱 她” 和 “她 爱 我” 就能区别开来了。当然啦，为了提高效率，我们需要过滤掉低频的 N-gram。</p><h1 id="三、总结"><a href="#三、总结" class="headerlink" title="三、总结"></a>三、总结</h1><h2 id="3-1-fastText和word2vec的区别"><a href="#3-1-fastText和word2vec的区别" class="headerlink" title="3.1 fastText和word2vec的区别"></a>3.1 fastText和word2vec的区别</h2><p>相似处：</p><ol><li>图模型结构很像，都是采用embedding向量的形式，得到word的隐向量表达。</li><li>都采用很多相似的优化方法，比如使用Hierarchical softmax优化训练和预测中的打分速度。</li></ol><p>不同处：</p><ol><li>模型的输出层：word2vec的输出层，对应的是每一个term，计算某term的概率最大；而fasttext的输出层对应的是 分类的label。不过不管输出层对应的是什么内容，起对应的vector都不会被保留和使用；</li><li>模型的输入层：word2vec的输出层，是 context window 内的term；而fasttext 对应的整个sentence的内容，包括term，也包括 n-gram的内容；</li></ol><p>两者本质的不同，体现在 h-softmax的使用：</p><ul><li>Wordvec的目的是得到词向量，该词向量 最终是在输入层得到，输出层对应的 h-softmax<br>也会生成一系列的向量，但最终都被抛弃，不会使用。</li><li>Fasttext则充分利用了h-softmax的分类功能，遍历分类树的所有叶节点，找到概率最大的label（一个或者N个）</li></ul><h2 id="3-2-小结"><a href="#3-2-小结" class="headerlink" title="3.2 小结"></a>3.2 小结</h2><p>总的来说，fastText的学习速度比较快，效果还不错。fastText适用与分类类别非常大而且数据集足够多的情况，当分类类别比较小或者数据集比较少的话，很容易过拟合。</p><p>可以完成无监督的词向量的学习，可以学习出来词向量，来保持住词和词之间，相关词之间是一个距离比较近的情况；<br>也可以用于有监督学习的文本分类任务，（新闻文本分类，垃圾邮件分类、情感分析中文本情感分析，电商中用户评论的褒贬分析）</p>]]></content>
      
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word2Vec</title>
      <link href="/2019/04/15/Word2Vec/"/>
      <url>/2019/04/15/Word2Vec/</url>
      
        <content type="html"><![CDATA[<blockquote><p>前人栽树，后人乘凉</p><p><a href="https://www.jianshu.com/p/14ec26a5892b" target="_blank" rel="noopener">word2vec、负采样、层序softmax</a></p><p><a href="https://www.jianshu.com/p/471d9bfbd72f" target="_blank" rel="noopener">通俗理解word2vec</a></p><p><a href="https://www.jianshu.com/p/cede3ae146bb" target="_blank" rel="noopener">不懂word2vec，还敢说自己是做NLP？</a> </p><p><a href="https://blog.csdn.net/itplus/article/details/37998797）" target="_blank" rel="noopener">word2vec 中的数学原理详解（五）基于 Negative Sampling 的模型</a> </p><p>最后一个讲的很细致，强烈推荐</p><p>动机：Word2Vec以及word Embedding概念很早就知道了，xc对此很感兴趣</p><p>不过最近发现一些公司面试时候喜欢考这里的知识点，然而层次softmax的细节以及如何应用到训练中我还不是很懂，于是找了一下相关资料</p></blockquote><h1 id="Word2Vec"><a href="#Word2Vec" class="headerlink" title="Word2Vec"></a>Word2Vec</h1><p>word2vec也叫word embeddings,中文名“词向量”、”词嵌入”。是Google 的 Tomas Mikolov 在《Efficient Estimation of Word Representation in Vector Space》提出的。word2vec分两个模型，分别为skip-gram(Continuous Skip-gram Model 跳字模型)和CBOW(Continuous Bag-of-Words Model 连续词袋模型)。 </p><p>本质上是一种单词聚类的方法 </p><h2 id="one-hot"><a href="#one-hot" class="headerlink" title="one-hot"></a>one-hot</h2><p>独热编码即 One-Hot 编码，又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都有它独立的寄存器位，并且在任意时候，其中只有一位有效。 </p><p><strong>优点</strong>：</p><ul><li><p>解决了分类器不好处理离散数据的问题</p></li><li><p>在一定程度上也起到了扩充特征的作用。</p></li></ul><p><strong>缺点</strong>：在文本特征表示上有些缺点就非常突出</p><ul><li>它是一个词袋模型，不考虑词与词之间的顺序（文本中词的顺序信息也是很重要的）</li><li>它假设词与词相互独立（在大多数情况下，词与词是相互影响的）</li><li>它得到的特征是离散稀疏的(维度灾难)。</li></ul><p><strong>解决：</strong></p><p>​    <strong>word embedding（词嵌入）</strong>，即将高维词向量嵌入到一个低维空间。 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g237ap3zgpj20d00awwfg.jpg" alt></p><p>词向量：</p><script type="math/tex; mode=display">\vec {King} - \vec {Man} + \vec {Woman} = \vec {Queen}</script><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>word2vec模型其实就是简单化的神经网络。 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g237gckx5oj20im0citc7.jpg" alt></p><p>输入是One-Hot Vector，Hidden Layer没有激活函数，也就是线性的单元。Output Layer维度跟Input Layer的维度一样，用的是Softmax回归。当这个模型训练好以后，我们并不会用这个训练好的模型处理新的任务，我们真正需要的是这个模型通过训练数据所学得的参数，例如隐层的权重矩阵。 </p><p><strong>每个词都可有两个向量表达，既可以是背景词，也可以是中心词。</strong> </p><p><strong>词袋模型训练时间更短，但是skip-gram更精准</strong> </p><p><strong>CBOW对小型数据库比较合适，而Skip-Gram在大型语料中表现更好</strong> </p><h2 id="skip-gram"><a href="#skip-gram" class="headerlink" title="skip-gram"></a>skip-gram</h2><p>跳字模型就是用一个词去预测它文本序列周围的词，例如：”the“，”man“，”hit“，”his“，”son“。比如时间窗口大小为2，则可以用”hit“中心词来预测”the“，”man“，”his“，”son“这些窗口小于2的背景词的最大概率。 </p><h2 id="CBOW"><a href="#CBOW" class="headerlink" title="CBOW"></a>CBOW</h2><p>连续词袋模型就是用文本序列周围的词来预测中心词，例如，给定文本序列”the“，”man“，”hit“，”his“，”son“，连续词袋模型关心的是给定”the“，”man“，”his“，”son“一起生成中心词”hit“的概率。 </p><h2 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h2><p><strong>以“我爱北京天安门”这句话为例。</strong></p><ol><li><p>假设我们现在关注的词是“爱”，</p></li><li><p>C＝2时它的上下文分别是“我”，“北京天安门”</p></li><li><p>CBOW模型就是把“我” “北京天安门” 的one hot表示方式作为输入，也就是C个$1*V$的向量</p></li><li><p>分别跟同一个$VxN$的大小的系数矩阵$W1$相乘得到C个$1xN$的隐藏层$hidden layer$，</p></li><li><p>C个取平均所以只算一个隐藏层。</p><blockquote><p> 这个过程也被称为线性激活函数(这也算激活函数？分明就是没有激活函数了)。</p></blockquote></li><li><p>再跟另一个$NxV$大小的系数矩阵W2相乘得到$1xV$的输出层</p><blockquote><p>这个输出层每个元素代表的就是词库里每个词的事后概率。</p></blockquote></li><li><p>输出层需要跟ground truth也就是“爱”的one hot形式做比较计算loss。</p><p>优化：这里需要注意的就是V通常是一个很大的数比如几百万，计算起来相当费时间，除了“爱”那个位置的元素肯定要算在loss里面，word2vec就用基于huffman编码的<strong>Hierarchical softmax</strong>筛选掉了一部分不可能的词，然后又用<strong>nagetive samping</strong>再去掉了一些负样本的词所以时间复杂度就从O(V)变成了O(logV)。Skip gram训练过程类似，只不过输入输出刚好相反。 </p></li></ol><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g23bzys94yj20om0epgln.jpg" alt></p><p><strong>知乎的回答：</strong></p><ol><li>输入层：上下文单词的onehot. {假设单词向量空间dim为V，上下文单词个数为C}</li><li>所有onehot分别乘以共享的输入权重矩阵W. {V*N矩阵，N为自己设定的数，初始化权重矩阵W}</li><li>所得的向量 {因为是onehot所以为向量} 相加求平均作为隐层向量, size为1*N.</li><li>乘以输出权重矩阵W’ {N*V}</li><li>得到向量 {1*V} 激活函数处理得到V-dim概率分布 {PS: 因为是onehot嘛，其中的每一维斗代表着一个单词}，概率最大的index所指示的单词为预测出的中间词（target word）</li><li>与true label的onehot做比较，误差越小越好</li></ol><blockquote><p>所以，需要定义loss function（一般为交叉熵代价函数），采用梯度下降算法更新W和W’。训练完毕后，输入层的每个单词与矩阵W相乘得到的向量的就是我们想要的词向量（word embedding），这个矩阵（所有单词的word embedding）也叫做look up table（其实聪明的你已经看出来了，其实这个look up table就是矩阵W自身），也就是说，任何一个单词的onehot乘以这个矩阵都将得到自己的词向量。有了look up table就可以免去训练过程直接查表得到单词的词向量了。<em>**</em></p></blockquote><p><strong>训练加速小trick</strong></p><ol><li>把常见的词组作为一个单词。  </li><li>少采样常见的词 （译者按：A the 啥的都滚粗）  </li><li>修改优化目标函数，这个策略成为“Negative Sampling“，使得每个训练样本只去更新模型中一小部分的weights </li></ol><p><strong>普遍认为Hierarchical Softmax对低频词效果较好；Negative Sampling对高频词效果较好，向量维度较低时效果更好</strong> </p><h2 id="降采样（下采样）"><a href="#降采样（下采样）" class="headerlink" title="降采样（下采样）"></a>降采样（下采样）</h2><p>又名downsampled、SubSampling </p><p>实际是对高频词进行随机采样，关于随机采样的选择问题，考虑高频词往往提供相对较少的信息，因此可以将高于特定词频的词语丢弃掉，以提高训练速度。 </p><p>解决“the“这种常见的词，（理解：数据不平衡）</p><p>我们使用 $w_i$ 来表示单词，$z(w_i)$ 表示它出现在词库中的概率。比如花生在1bilion的词库中出现了1,000词，那么$z(花生)=1E−6$。 </p><p>有个叫‘sample‘的参数控制了降采样的程度，一般设置为0.001。这个值越小代表更容易扔掉一些词。 </p><script type="math/tex; mode=display">P(w_i)=(\sqrt {(z(w_i)/0.001}+1)×0.001/z(w_i)</script><p>$z(w_i)&lt;=0.0026$ 的情况下，$P(w_i)=1$，我们不会把这些词扔掉</p><p>当$z(w_i)&lt;=0.00746$ 的情况下，$P(wi)=0.5$,</p><h2 id="负采样Negative-Sampling（skip-gram举例）"><a href="#负采样Negative-Sampling（skip-gram举例）" class="headerlink" title="负采样Negative Sampling（skip-gram举例）"></a>负采样Negative Sampling（skip-gram举例）</h2><p>训练神经网络 意味着输入一个训练样本调整weight，让它预测这个训练样本更准。换句话说，每个训练样本将会影响网络中所有的weight。</p><p>像我们之前讨论的一样，我们词典的大小意味着我们有好多weight，所有都要轻微的调整。</p><blockquote><p>negative sampling 每次让一个训练样本仅仅更新一小部分的权重参数，从而降低梯度下降过程中的计算量。 </p><p>如果 vocabulary 大小为1万时， 当输入样本 ( “fox”, “quick”) 到神经网络时，</p><p> “ fox” 经过 one-hot 编码，在输出层我们期望对应 “quick” 单词的那个神经元结点输出 1，</p><p>其余 9999 个都应该输出 0。</p><p>在这里，这9999个我们期望输出为0的神经元结点所对应的单词我们为 negative word.  </p></blockquote><p>Negative sampling 解决了这个问题，每次我们就修改了其中一小部分weight，而不是全部。</p><p>当训练（fox，quick）这个词对的时候，quick这个词的概率是1，其他的都是0。通过negative sample，我们只是随机的选了一部分negative词（假设是5个）来update weight。（这些negative 词就是我们希望是0的。）</p><blockquote><p>论文中说5-20个词适合小数据集， 2-5个词适合大数据集。</p></blockquote><p>negative sample也是根据他们出现频率来选的。更常出现的词，更容易被选为negative sample。 </p><script type="math/tex; mode=display">P(w_i) = \frac{f(w_i)^{0.75}}{\sum_{j=0}^n (f(w_j)^{0.75})}</script><p>$f(w) $代表 每个单词被赋予的一个权重，即 它单词出现的词频.</p><p><strong>与层次softmax相比，NEG不再使用复杂的Huffman树，而是利用相对简单的随机负采样，能大幅提高性能，因为可以作为分层softmax的一种替代。</strong></p><h2 id="层次softmax-Hierarchical-Softmax"><a href="#层次softmax-Hierarchical-Softmax" class="headerlink" title="层次softmax Hierarchical Softmax"></a>层次softmax Hierarchical Softmax</h2><p>大家都知道<strong>哈夫曼树</strong>是带权路径最短的树，一般神经网络语言模型在预测的时候，输出的是预测目标词的概率（每一次预测都要基于全部的数据集进行计算，很大的时间开销）。 </p><p>Hierarchical Softmax是一种对<strong>输出层进行优化</strong>的策略:</p><p>输出层从原始模型的利用softmax计算概率值改为了利用Huffman树计算概率值。</p><p>一开始我们可以用以词表中的全部词作为叶子节点，词频作为节点的权，构建Huffman树，作为输出。从根节点出发，到达指定叶子节点的路径是唯一的。Hierarchical Softmax正是利用这条路径来计算指定词的概率，而非用softmax来计算。</p><p> 即Hierarchical Softmax：把 N 分类问题变成 log(N)次二分类 </p><hr><p>作为一种计算高效的近似方法，Hierarchical Softmax被广泛使用。Morin和Bengio[1]首先将这种方法引入神经网络语言模型。 </p><p>该方法不用为了获得概率分布而评估神经网络中的W个输出结点，而只需要评估大约log2(W)个结点。层次Softmax使用一种二叉树结构来表示词典里的所有词，V个词都是二叉树的叶子结点，而这棵树一共有V−1个非叶子结点。 </p><p>对于每个叶子结点（词），总有一条从根结点出发到该结点的唯一路径。这个路径很重要，因为要靠它来估算这个词出现的概率。以下图为例（图来自Xin Rong,2014[6]），白色结点为词典中的词，深色是非叶子结点。图中画出了从根结点到词w2的唯一路径，路径长度L(w2)=4，而n(w,j)表示从根结点到词w2的路径上的的第j个结点。 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g23g469v0ej20u20fgwhr.jpg" alt></p><p>在层次Softmax模型中，叶子节点的词没有直接输出的向量，而非叶子节点其实都有响应的输出向量。<em>**</em> </p><p><strong>求目标词的概率</strong> ：</p><p>在模型的训练过程中，通过Huffman编码，构造了一颗庞大的Huffman树，同时会给非叶子结点赋予向量。我们要计算的是目标词w的概率，这个概率的具体含义，是指从root结点开始随机走，走到目标词w的概率。因此在途中路过非叶子结点（包括root）时，需要分别知道往左走和往右走的概率。 </p><h2 id="word2vec和word-embedding的区别"><a href="#word2vec和word-embedding的区别" class="headerlink" title="word2vec和word embedding的区别"></a>word2vec和word embedding的区别</h2><p>简言之，word embedding 是一个将词向量化的概念，中文译名为”词嵌入”。 word2vec是谷歌提出的一种word embedding的具体手段，采用了两种模型(CBOW与skip-gram模型)与两种方法(负采样与层次softmax方法)的组合，比较常见的组合为 skip-gram+负采样方法。 </p>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>醉里论道 醒时折花</title>
      <link href="/2019/04/14/your-poetry-name/"/>
      <url>/2019/04/14/your-poetry-name/</url>
      
        <content type="html"><![CDATA[<p>宁向直中取,不向曲中求</p><p>曲终人不见，江上数峰青</p><p>一抔之土未干,六尺之孤安在</p><p>既往不恋,未来不迎,当下不杂.</p><p>大漠孤烟直，长河落日圆</p><p>当时明月在 曾照彩云归</p><p>灿如春华，姣如秋月</p><p>白首如新，倾盖如故</p><p>水平以鉴 火静而朗</p><p>霜雪落满头，也算到白首</p><p>南有乔木，不可休思</p><p>是耶非耶，化为蝴蝶</p><p>魂魄毅兮为鬼雄</p><p>未来不迎 当下不杂 既往不恋</p><p>昨日之日不可追，明日之日须臾期</p><p>百川到海，何时西归？ 日月既往，不可复追</p><p>一如月下山岗，何惧八面来风</p><p>执一笙念</p><p>霜华苍苍，白衣未染</p><p>扶刀思壮志，望雁泪千行</p><p>画的青山眉样好，百年有结是同心。</p><p>一任皓雪落江湖，白衣何必知寒暑</p><p>天光乍破遇，暮雪白头老。</p><p>渐行渐远渐无书　水阔鱼沉何处问</p><p>重过阊门万事非，同来何事不同归。</p><p>背灯和月就花阴，已是十年踪迹十年心。</p><p>入我相思门，知我相思苦。</p><p>长相思兮长相忆，短相思兮无穷极。</p><p>早知如此绊人心，何如当初莫相识。</p><p>自在飞花轻似梦，无边丝雨细如愁。</p><p>茕茕白兔 东走西顾 衣不如新 人不如故</p><p>天上白玉京，五楼十二城。仙人抚我顶，结发受长生。</p><p>始知断袖怯惊梦 方晓情重溢于言</p><p>关山难越，谁悲识路之人。萍水相逢，尽是他乡之客。</p><p>今夕何夕兮？搴舟中流；今日何日兮？得与王子同舟。蒙羞被好兮，不訾诟耻。心几烦而不绝兮，知得王子。山有</p><p>木兮木有枝，心悦君兮君不知。</p><p>无人与我立黄昏 无人问我粥可温</p><p>无人与我捻熄灯 无人共我书半生</p><p>夫天地者，万物之逆旅也；光阴者，百代之过客也。而浮生若梦，为欢几何？西风多少恨 吹不散眉弯</p><p>遇人三分冷暖，知心十分真情</p><p>青山秀水纸上栽，一叶轻舟过江来。白墙朱瓦哺奇秀，我辈镂骨育英才。</p><p>近来寒暑不常，希自珍慰</p><p>烛明香暗画堂深，满鬓青霜残雪思前任</p><p>我年轻的时候，也曾快马加鞭，看尽长安花</p><p>飘飘不知何所至，怅惘苍穹薄日溃。</p><p>年年最喜风雪时，放马长歌博一醉</p><p>男儿从来不恤身,纵死敌手笑相承,仇场仗场一百处,处处愿与野草青出自哪里</p><p>不是粉黛而颜色如朝霞映雪</p><p>青青蛇儿口，黄蜂尾上针。</p><p>两者俱不毒，最毒女人心。</p><p>海棠不惜胭脂色，独立濛濛细雨中</p><p>一丝轻雷落万丝，霁光浮瓦碧参差。</p><p>有情芍药含春泪，无力蔷薇卧晓枝。</p><p>腾云似涌烟，密雨如散丝。</p><p>残虹收度雨，缺岸上新流。</p><p>细雨湿衣看不见，闲花落地听无声。</p><p>涧底松摇千尺雨，庭中竹撼一窗秋。</p><p>春雨断桥人不度，小舟撑出柳阴来</p><p>玉容寂寞泪阑干，梨花一支春带雨。</p><p>小楼一夜听春雨，深巷明朝卖杏花。</p><p>深秋帘幕千家雨，落日楼台一笛风</p><p>赋一篇诗意，让梅兰竹菊同妍</p><p>洒满心柔情,将万紫千红开遍华灯初上暮色四合</p><p>故其疾如风,其徐如林,侵掠如火</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>CNN 多通道卷积核 channel and conv</title>
      <link href="/2019/04/11/CNN%20channel%20and%20conv/"/>
      <url>/2019/04/11/CNN%20channel%20and%20conv/</url>
      
        <content type="html"><![CDATA[<blockquote><p>自己看的记录笔记</p><p>写的很差 勿看</p><p>参考文献</p><p>好多博客都是类似的内容 希望没有写错</p><p><a href="https://blog.csdn.net/jacke121/article/details/80188821" target="_blank" rel="noopener">https://blog.csdn.net/jacke121/article/details/80188821</a> </p><p><a href="https://blog.csdn.net/haoji007/article/details/81981846" target="_blank" rel="noopener">https://blog.csdn.net/haoji007/article/details/81981846</a> </p></blockquote><h1 id="CNN-多通道卷积计算理解"><a href="#CNN-多通道卷积计算理解" class="headerlink" title="CNN 多通道卷积计算理解"></a>CNN 多通道卷积计算理解</h1><p>对于单通道图像，若利用10个卷积核进行卷积计算，可以得到10个特征图；若输入为多通道图像，则输出特征图的个数依然是卷积核的个数（10个）。 </p><p>以图片来举例：假设图片的宽度为width:W，高度为height:H，图片的通道数为D，一般目前都用RGB三通道D=3，为了通用性，通道数用D表示； </p><p>卷积核：卷积核大小为K*K，由于处理的图片是D通道的，因此卷积核其实也就是K*K*D大小的，因此，对于RGB三通道图像，在指定kernel_size的前提下，真正的卷积核大小是kernel_size<em>kernel_size\</em>3。 </p><p><strong>对于D通道图像的各通道而言，是在每个通道上分别执行二维卷积，然后将D个通道加起来，得到该位置的二维卷积输出</strong></p><p>对于RGB三通道图像而言，就是在R，G，B三个通道上分别使用对应的每个通道上的kernel_size*kernel_size大小的核去卷积每个通道上的W*H的图像，然后将三个通道卷积得到的输出相加，得到二维卷积输出结果。因此，若有M个卷积核，可得到M个二维卷积输出结果，在有padding的情况下，能保持输出图片大小和原来的一样，因此是output(W,H,M)。</p><p>下面的图动态形象地展示了三通道图像卷积层的计算过程：</p><p>下图是7<em>7</em>3的图像，3通道，有2个3<em>3</em>3的卷积核，也称3*2=6个卷积核</p><p><img src="https://segmentfault.com/img/bVW1tf?w=860&amp;h=690" alt="å¾çæè¿°"> </p><p>有教程为7<em>7</em>M通道的图片，输出通道是n，则卷积核共m*n个卷积核。</p><p>原版动图地址：</p><p><a href="http://cs231n.github.io/convolutional-networks/" target="_blank" rel="noopener">http://cs231n.github.io/convolutional-networks/</a></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">四个通道上的卷积操作:</span><br><span class="line">有两个卷积核，生成两个通道。</span><br><span class="line">其中需要注意的是，四个通道上每个通道对应一个2*2的卷积核</span><br><span class="line">这4个2*2的卷积核上的参数是不一样的，之所以说它是1个卷积核，是因为把它看成了一个4*2*2的卷积核，4代表一开始卷积的通道数，2*2是卷积核的尺寸，实际卷积的时候其实就是4个2*2的卷积核（这四个22的卷积核的参数是不同的）分别去卷积对应的4个通道，然后相加，再加上偏置b，注意b对于这四通道而言是共享的，所以b的个数是和最终的featuremap的个数相同的，然后再取激活函数</span><br><span class="line">    </span><br><span class="line">输出层的卷积核个数为 feature map 的个数。也就是说卷积核的个数=最终的featuremap的个数，卷积核的大小=开始进行卷积的通道数每个通道上进行卷积的二维卷积核的尺寸（此处就是4（2*2）），b（偏置）的个数=卷积核的个数=featuremap的个数。</span><br><span class="line"></span><br><span class="line">4个通道卷积得到2个通道的过程中，参数的数目为4×（2×2）×2+2个，其中4表示4个通道，第一个2*2表示卷积核的大小，第三个2表示生成的featuremap个数，也就是得到的2通道的feature map，也就是生成的通道数，最后的2代表偏置b的个数。</span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
            <tag> CNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>self-attention 相对位置 relative position.</title>
      <link href="/2019/04/08/%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%20relative%20position/"/>
      <url>/2019/04/08/%E7%9B%B8%E5%AF%B9%E4%BD%8D%E7%BD%AE%20relative%20position/</url>
      
        <content type="html"><![CDATA[<blockquote><p>自己看的记录笔记</p><p>写的很差 勿看</p><p>参考文献</p><p><a href="https://blog.csdn.net/luoxiaolin_love/article/details/82258069" target="_blank" rel="noopener">https://blog.csdn.net/luoxiaolin_love/article/details/82258069</a> </p></blockquote><h1 id="相对位置"><a href="#相对位置" class="headerlink" title="相对位置"></a>相对位置</h1><p>NAACL 2018的论文 </p><p>读论文读的太少了</p><blockquote><p>作者认为正弦余弦位置向量效果是比可学习的更好的，因为他可以看到训练中未见到的序列长度</p><p>self attention中增加位置感知，考虑输入元素之间的成对关系 ，窗口 不用全看 多一个参与学习的矩阵 </p><p>对于长度为n的，有h个attention heads的序列，通过在每个head上共享相对位置表示 </p><p>why 相对位置：输入元素间的成对关系</p></blockquote><h2 id="1-Transformer"><a href="#1-Transformer" class="headerlink" title="1. Transformer"></a>1. Transformer</h2><p>Transformer采用由堆叠编码器和解码器层组成的编码器-解码器结构。编码器层由两个子层组成：self-attention层，然后是一个位置感知的前馈层。解码器层由三个子层组成：self-attention，然后是编码器-解码器attention，然后是一个位置感知的前馈层。它使用每个子层周围的残差连接，然后再进行层归一化。解码器在其self-attention中使用掩码以防止给定的输出位置在训练期间包含关于未来输出位置的信息。</p><p>在第一层之前，在编码器和解码器输入元件中添加基于频率变化的正弦信号的位置编码。与学习的绝对位置表示相比，<strong>作者假设正弦波位置编码将帮助模型推广到训练过程中未见的序列长度，使它能够学习参加相对位置，这一性质是由作者的相对位置表示所共有的</strong>，与绝对位置表示相比，相对位置表示与总序列长度是不一致的。</p><p> 残差连接有助于将位置信息传播到更高的层。</p><p>修剪最大距离还使模型能够推广到训练期间未观察到的序列长度。因此，作者考虑2k+1唯一的边缘标签 </p><hr><h2 id="CSDN-博客"><a href="#CSDN-博客" class="headerlink" title="CSDN 博客"></a>CSDN 博客</h2><p>在《Attention Is All You Need》基础上，对position encodeing进行了优化。在《Attention Is All You Need》中采用的position encoding的方式，没有考虑到词与词之间的距离。因此本文提出一个位置向量表示<img src="https://img-blog.csdn.net/20180831183033560?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">与<img src="https://img-blog.csdn.net/20180831183102452?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">，分别表示输入序列第i个元素与第j个元素的Key和Value对应的位置向量。因此，输出元素zi的计算为：</p><p><img src="https://img-blog.csdn.net/20180831183256789?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180831183325257?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p><img src="https://img-blog.csdn.net/20180831183336288?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>​      对于现行序列，本文边可以捕获输入元素之间相对位置的差异性。因为本文假设一定距离之外精确的相对位置信息在是无用的，所以只考虑最大相对位置为k的情况。这种使用j-i的形式，可以处理在训练集中没有见过的序列长度。(之前的方法应该存在这个问题，不能处理比训练数据更长的数据)</p><p><img src="https://img-blog.csdn.net/20180831183613197?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p><p>而 <img src="https://img-blog.csdn.net/20180831183033560?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">与<img src="https://img-blog.csdn.net/20180831183102452?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img">的计算如下：</p><p><img src="https://img-blog.csdn.net/20180831183939962?watermark/2/text/aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2x1b3hpYW9saW5fbG92ZQ==/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70" alt="img"></p>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> Transformer </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize</title>
      <link href="/2019/03/18/%E6%A2%AF%E5%BA%A6%E5%BC%A5%E6%95%A3%20%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%20%E8%BF%87%E6%8B%9F%E5%90%88/"/>
      <url>/2019/03/18/%E6%A2%AF%E5%BA%A6%E5%BC%A5%E6%95%A3%20%E6%A2%AF%E5%BA%A6%E7%88%86%E7%82%B8%20%E8%BF%87%E6%8B%9F%E5%90%88/</url>
      
        <content type="html"><![CDATA[<blockquote><p>自己看的记录笔记</p><p>参考文献</p><p><a href="https://blog.csdn.net/zhangbaoanhadoop/article/details/82290129" target="_blank" rel="noopener">解决梯度消失和梯度弥散的方法</a> </p><p><a href="https://www.cnblogs.com/robert-dlut/p/5952032.html/" target="_blank" rel="noopener">注意力机制（Attention Mechanism）在自然语言处理中的应用</a></p><p><a href="https://zhuanlan.zhihu.com/p/40920384" target="_blank" rel="noopener">真正的完全图解Seq2Seq Attention模型</a></p><p><a href="https://blog.csdn.net/u010089444/article/details/76725843" target="_blank" rel="noopener">优化方法总结：SGD，Momentum，AdaGrad，RMSProp，Adam</a></p><p><a href="https://blog.csdn.net/shuzfan/article/details/75675568" target="_blank" rel="noopener">梯度下降优化算法总结</a></p></blockquote><h1 id="深度学习概念"><a href="#深度学习概念" class="headerlink" title="深度学习概念"></a>深度学习概念</h1><h2 id="梯度弥散-梯度爆炸"><a href="#梯度弥散-梯度爆炸" class="headerlink" title="梯度弥散 梯度爆炸"></a>梯度弥散 梯度爆炸</h2><p>​    梯度弥散。使用反向传播算法传播梯度的时候，随着传播深度的增加，梯度的幅度会急剧减小，会导致浅层神经元的权重更新非常缓慢，不能有效学习。这样一来，深层模型也就变成了前几层相对固定，只能改变最后几层的浅层模型。  </p><p>​    梯度弥散的问题很大程度上是来源于激活函数的“饱和”。 </p><p><strong>梯度消失：</strong></p><blockquote><ol><li>Relu代替sigmoid(梯度消失 )</li><li>梯度裁剪、正则(损失函数、针对梯度爆炸 )</li><li>RNN-LSTM</li><li>一种新的方法batch normalization</li><li>ResNet残差结构</li><li>预训练+微调</li></ol></blockquote><p><strong>梯度爆炸：</strong></p><blockquote><p> 使用Gradient Clipping(梯度裁剪)。通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。 </p><p>原来的网络，如果简单地<strong>增加深度</strong>，会导致梯度弥散或梯度爆炸。对于该问题的解决方法是正则化初始化和中间的正则化层（Batch <strong>Normalization</strong>），这样的话可以训练几十层的网络。</p><p>虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。</p><p><strong>residual</strong></p></blockquote><h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。</p><p>为了防止overfitting，可以用的方法有很多，下文就将以此展开。有一个概念需要先说明，在机器学习算法中，我们常常将原始数据集分为三部分：training data、validation data，testing data。这个validation data是什么？它其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。那为啥不直接在testing data上做这些呢？因为如果在testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的testing data，导致最后得到的testing accuracy没有任何参考意义。因此，training data的作用是计算梯度更新权重，validation data如上所述，testing data则给出一个accuracy以判断网络的好坏。</p><p>避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout。</p><h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul><li>所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集 上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。 </li><li>当一个模型过为复杂之后，它可以很好的记忆每一个训练数据中的随机噪声，却忘记要去学习训练数据中的通用趋势</li></ul><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul><li>过拟合其中一个可能的成因就是模型的vc维过高，使用了过强的模型复杂度(model complexity)的能力。（参数多并且过训练）  　　</li><li>还有一个原因是数据中的噪声，造成了如果完全拟合的话，也许与真实情景的偏差更大。  　　</li><li>最后还有一个原因是数据量有限，这使得模型无法真正了解整个数据的真实分布。  　　<ul><li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。     </li></ul></li></ul><h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol><li><p>权值衰减  　　在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。<strong>（L2正则化）</strong>  </p></li><li><p>适当的stopping criterion（验证集） </p></li><li><p><strong>交叉验证方法</strong></p><p>在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重 </p><p><strong>k-fold交叉方法</strong>:  　　把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例;每次实验中,使用上面讨论的交叉验证过程来决定在验证集合上取得最佳性能的迭代次数n*,然后计算这些迭代次数的均值,作为最终需要的迭代次数。 </p></li><li><p><strong>正则化</strong> </p><p>​    正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。 </p><p>​    L1和L2正则都是比较常见和常用的正则化项，都可以达到<strong>防止过拟合</strong>的效果。L1正则化的解具有<strong>稀疏性</strong>，可用于<strong>特征选择</strong>。L2正则化的解都比较小，<strong>抗扰动能力强</strong>。 </p><p>​    <strong>使用L2正则项的解不具有稀疏性</strong> ，L2缺点：模型的可解释性。它将把不重要的预测因子的系数缩小到趋近于 0，但永不达到 0。也就是说，最终的模型会包含所有的预测因子</p></li></ol><p>   ​    正则化方法是在损失函数时候改变</p><script type="math/tex; mode=display">   J(\theta) = J(\theta) + \lambda R(w)</script><script type="math/tex; mode=display">   R(w) = ||w||_1=\sum_{i}{|w_i|}</script><script type="math/tex; mode=display">   R(w) = ||w||_2^2=\sum_{i}{|w_i^2|}   \text {L2正则}</script><p>   ​    通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。</p><p>   ​    <strong>L2 regularizer</strong> ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。</p><p>   ​    <strong>L1 regularizer</strong> ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。</p><p>   L1 的优点: 能够获得更加稀疏的模型.<br>   L1 的缺点: 加入 L1 后会使得目标函数在原点不可导, 需要做特殊处理</p><p>   L2 的有点: 在任意位置都可导, 优化求解过程比较方便, 而且更加稳定<br>   L2 的缺点: 无法获得真正的稀疏模型</p><p>   <strong>在实际应用过程中, 大部分情况下都是 L2 正则的效果更好, 因此推荐优先使用 L2 正则</strong></p><p>   <strong>正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</strong> </p><ol><li><p><strong>Dropout正则化</strong></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">部分置为：0</span><br><span class="line">其他缩放：1 / (1 - rate)</span><br></pre></td></tr></table></figure></li><li><p>数据！</p><blockquote><p>（1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。</p><p>（2）选取合适的停止训练标准，使对机器的训练在合适的程度；</p><p>（3）保留验证数据集，对训练成果进行验证；</p><p>（4）获取额外数据进行交叉验证；</p><p>（5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等</p></blockquote></li></ol><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ul><li>基于模型的方法: 采用降低过拟合风险的措施,包括简化模型(如将非线性简化成线性), 添加约束项以缩小假设空间(如L1和L2正则化), 集成学习, Dropout超参数等.</li><li>基于数据的方法, 主要通过数据扩充(Data Augmentation), 即根据一些先验知识, 在保持特定信息的前提下, 对原始数据进行适合变换以达到扩充数据集的效果.</li></ul><h2 id="Batch-size-amp-SGD"><a href="#Batch-size-amp-SGD" class="headerlink" title="Batch size&amp;SGD"></a>Batch size&amp;SGD</h2><h3 id="Batch-Size三种情况（SGD）"><a href="#Batch-Size三种情况（SGD）" class="headerlink" title="Batch_Size三种情况（SGD）"></a>Batch_Size三种情况（SGD）</h3><p>　　Batch_Size（批尺寸）是机器学习中一个重要参数。 </p><ol><li><p><strong>Batch Gradient Descent</strong></p><p>如果数据集比较小，完全可以采用全数据集 （ Full Batch Learning ）的形式，这样做至少有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 </p><blockquote><ul><li><p>凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点 </p></li><li><p>每次学习时间过长 </p></li><li>训练集很大以至于需要消耗大量的内存 </li><li>全量梯度下降不能进行在线模型参数更新 </li></ul></blockquote></li><li><p><strong>Stochastic Gradient Descent</strong></p><p>Batch_Size = 1。这就是在线学习（Online Learning）。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞<a href="https://www.baidu.com/s?wd=%E5%90%84%E8%87%AA%E4%B8%BA%E6%94%BF&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">各自为政</a>，难以达到收敛。 </p><blockquote><ul><li>算法收敛速度快 </li><li><p>可以在线更新</p></li><li><p>最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动) </p></li><li>容易收敛到局部最优，并且容易被困在鞍点</li></ul></blockquote></li><li><p><strong>Mini-batch Gradient Descent</strong></p><p>如果网络中采用minibatch SGD算法来优化，所以是一个batch一个batch地将数据输入CNN模型中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。而batch_size就是一个batch的所包含的样本数，显然batch_size将影响到模型的优化程度和速度。<strong>mini batch只是为了充分利用GPU memory而做出的妥协</strong> </p><blockquote><ul><li><p>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</p></li><li><p>学习速率调整(又称学习速率调度，Learning rate schedules)[11]试图在每次更新过程中，改变学习速率，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点[10]。</p></li><li><p>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。</p></li><li><p>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。</p></li></ul></blockquote></li></ol><h3 id="改变Batch-Size的影响"><a href="#改变Batch-Size的影响" class="headerlink" title="改变Batch_Size的影响"></a>改变Batch_Size的影响</h3><p>　　在合理范围内，增大 Batch_Size 的好处：内存利用率提高了，大矩阵乘法的并行化效率提高。跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<strong>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小</strong>。 　　</p><blockquote><p>下面是参考文献中博主给出的实验结果： </p><p>Batch_Size 太小，算法在 200 epoches 内不收敛。 </p><p>随着 Batch_Size 增大，处理相同数据量的速度越快。 </p><p>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</p><p>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 </p><p>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</p></blockquote><p>　　当采用mini-batch时，我们可以将一个batch里的所有样本放在一个矩阵里，利用线性代数库来加速梯度的计算，这是工程实现中的一个优化方法。<br>　　一个大的batch，可以充分利用矩阵、线性代数库来进行计算的加速，batch越小，则加速效果可能越不明显。当然batch也不是越大越好，太大了，权重的更新就会不那么频繁，导致优化过程太漫长。</p><h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>SGD方法的一个缺点是，其<strong>更新方向</strong>完全依赖于当前的batch，因而其更新十分不稳定。</p><p>momentum，动量，它<a href="https://www.baidu.com/s?wd=%E6%A8%A1%E6%8B%9F&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">模拟</a>的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。</p><p>利用惯性，即当前梯度与上次梯度进行加权，如果方向一致，则累加导致更新步长变大；如果方向不同，则相互抵消中和导致更新趋向平衡。</p><p>动量参数常被设定为0.9或者一个相近的值。</p><blockquote><p>Nesterov Momentum</p><p>在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在<strong>凸优化</strong>中有较强的理论保证收敛。 </p></blockquote><h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新</p><p>适合处理<strong>稀疏数据</strong> </p><p>对角矩阵 每个对角线位置是1-&gt;t轮梯度平方和</p><p>缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束 </p><h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta： <strong>仅采用一个窗口范围内的梯度平方之和</strong> </p><p><strong>Adadelta已经无需设置初始学习率了，其可以自动计算并更新学习率</strong> </p><h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题 </p><h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><blockquote><p>梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum） </p><p>​     1.Adam算法可以看做是修正后的<a href="http://blog.csdn.net/bvl10101111/article/details/72615621" target="_blank" rel="noopener">Momentum</a>+<a href="http://blog.csdn.net/BVL10101111/article/details/72616378" target="_blank" rel="noopener">RMSProp</a>算法</p><p>​     2.动量直接并入梯度一阶矩估计中（指数加权）</p><p>​     3.Adam通常被认为对超参数的选择相当鲁棒</p><p>​     4.学习率建议为0.001</p></blockquote><p>Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率 </p><p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 </p><p><strong>Adam = Momentum+RMSProp</strong>  一阶 + 二阶</p><p>度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑</p><blockquote><ol><li><p>实现简单，计算高效，对内存需求少</p></li><li><p>参数的更新不受梯度的伸缩变换影响</p></li><li><p>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</p></li><li><p>更新的步长能够被限制在大致的范围内（初始学习率）</p></li><li><p>能自然地实现步长退火过程（自动调整学习率）</p></li><li><p>很适合应用于大规模的数据及参数的场景</p></li><li><p>适用于不稳定目标函数</p></li><li><p>适用于梯度稀疏或梯度存在很大噪声的问题</p></li></ol><p>综合Adam在很多情况下算作默认工作性能比较优秀的优化器。</p><p>warmup 更新学习率</p><p>Adam 快！</p></blockquote><h2 id="attention-乘法-加法"><a href="#attention-乘法-加法" class="headerlink" title="attention 乘法 加法"></a>attention 乘法 加法</h2><blockquote><p>所谓乘法 加法 做的是decoder的隐藏状态S_{t-1}和encoder的各个隐层状态h_j</p><p>这里</p></blockquote><script type="math/tex; mode=display">P(y_i|y_1,...y_{i-1},X) = g(y_{i-1},s_i,c_i)</script><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g1zvqx6ma6j20di07u0to.jpg" alt> <img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g1zvsjhqjej20du07ewff.jpg" alt></p><h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>使用sigmod控制</p><h2 id="数据不平衡"><a href="#数据不平衡" class="headerlink" title="数据不平衡"></a>数据不平衡</h2><p><strong>大数据+分布均衡&lt;大数据+分布不均衡&lt;小数据+数据均衡&lt;小数据+数据不均衡</strong> </p><p>解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类” </p><ol><li><p><strong>采样</strong></p><p>采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。 </p><p><strong>上采样（Oversampling）</strong>和<strong>下采样（Undersampling）</strong>，上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。 </p><p>上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；</p><ol><li>可以在每次生成新数据点时加入轻微的随机扰动 </li></ol><p>下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。 </p><ol><li>第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。</li><li>第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。</li><li>第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大 </li></ol></li><li><p><strong>数据合成</strong></p></li><li><p><strong>加权</strong></p></li></ol><ol><li><strong>一分类</strong></li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer norm 先做后做</title>
      <link href="/2019/03/14/Transformer%20norm%20%E5%85%88%E5%81%9A%E5%90%8E%E5%81%9A/"/>
      <url>/2019/03/14/Transformer%20norm%20%E5%85%88%E5%81%9A%E5%90%8E%E5%81%9A/</url>
      
        <content type="html"><![CDATA[<blockquote><p>并不适合阅读的个人文档。</p></blockquote><h1 id="Normalization-先做-和-后做"><a href="#Normalization-先做-和-后做" class="headerlink" title="Normalization  先做 和 后做"></a>Normalization  先做 和 后做</h1><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g12hedto9jj20d208e3yt.jpg" alt></p><ul><li><p>后做是传统做法</p><p>后做是 input x ,  x residual 到后面, x 进行function(multi-head attention),  f(x) 进行dropout，df(x) + x  （residual）, layer normalization(df(x) + x  )</p><p><strong>x’-&gt;LN(df(x) + x)</strong></p></li><li><p>先做是当前性能好的做法</p><p>后做是 input x ,  x residual 到后面, x 进行LN，然后进行function(multi-head attention), 最后进行dropout在和残差相加，dfLN(x) + x  （residual）</p><p><strong>x‘-&gt;x + (df(LN(x))</strong></p><blockquote><p>最顶层+LN</p><p>反向传播更好</p></blockquote></li><li><p>思考</p><p>为何不使用 xl(residual) -&gt; F -&gt; LN -&gt; add(residual) -&gt;xl+1</p><script type="math/tex; mode=display">x_l(residual) -> F -> LN -> add(residual) ->x_{l+1}</script><p>因为这样的操作，送入F的x是在加和后没进行norm的，容易出现梯度消失/梯度爆炸问题</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TensorFlow函数</title>
      <link href="/2019/03/11/TensorFlow%E5%87%BD%E6%95%B0/"/>
      <url>/2019/03/11/TensorFlow%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p></blockquote><h1 id="TensorFlow函数"><a href="#TensorFlow函数" class="headerlink" title="TensorFlow函数"></a>TensorFlow函数</h1><h2 id="tf-tile"><a href="#tf-tile" class="headerlink" title="tf.tile"></a>tf.tile</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># key_masks：(N, T_k) N:batch_size T:maxlen</span></span><br><span class="line">tf.tile(key_masks, [num_heads, <span class="number">1</span>]) <span class="comment"># (h*N, T_k)</span></span><br></pre></td></tr></table></figure><p>在input的每一维 复制对应的次数</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tile([x, y], [a, b])</span><br><span class="line"><span class="comment"># x 复制 a 次</span></span><br><span class="line"><span class="comment"># y 复制 b 次</span></span><br></pre></td></tr></table></figure><h2 id="tf-reduce-sum"><a href="#tf-reduce-sum" class="headerlink" title="tf.reduce_sum"></a>tf.reduce_sum</h2><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.reduce_sum(keys, axis=<span class="number">-1</span>)</span><br><span class="line"><span class="comment"># input按照第n维加和 </span></span><br><span class="line"><span class="comment"># 感觉就是那一维度没了</span></span><br><span class="line"><span class="comment"># [N, T, num_units] -&gt; [N, T]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果没有第二个参数，就是对矩阵中所有元素求和</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduce_sum</span><span class="params">(input_tensor,</span></span></span><br><span class="line"><span class="function"><span class="params">               axis=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               keepdims=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               name=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               reduction_indices=None,</span></span></span><br><span class="line"><span class="function"><span class="params">               keep_dims=None)</span>:</span></span><br><span class="line">  <span class="string">"""Computes the sum of elements across dimensions of a tensor.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Reduces `input_tensor` along the dimensions given in `axis`.</span></span><br><span class="line"><span class="string">  Unless `keepdims` is true, the rank of the tensor is reduced by 1 for each</span></span><br><span class="line"><span class="string">  entry in `axis`. If `keepdims` is true, the reduced dimensions</span></span><br><span class="line"><span class="string">  are retained with length 1.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  If `axis` has no entries, all dimensions are reduced, and a</span></span><br><span class="line"><span class="string">  tensor with a single element is returned.</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h2 id="tf-arg-max"><a href="#tf-arg-max" class="headerlink" title="tf.arg_max"></a>tf.arg_max</h2><p>tf.argmax() 与 numpy.argmax() 方法的意思是一致的， 即：</p><pre><code>axis = 0 时       返回每一列最大值的位置索引axis = 1 时       返回每一行最大值的位置索引</code></pre><h2 id="tf-nn-embedding-lookup"><a href="#tf-nn-embedding-lookup" class="headerlink" title="tf.nn.embedding_lookup"></a>tf.nn.embedding_lookup</h2><p>tf.nn.embedding_lookup()就是根据input_ids中的id，寻找embeddings中的第id行。比如input_ids=[1,3,5]，则找出embeddings中第1，3，5行，组成一个tensor返回。</p><p>embedding_lookup不是简单的查表，id对应的向量是可以训练的，训练参数个数应该是 category num*embedding size，也就是说lookup是一种全连接层。</p><p>我的理解是表中数是参与训练的</p><blockquote><p>作者：大师鲁<br>来源：CSDN<br>原文：<a href="https://blog.csdn.net/laolu1573/article/details/77170407" target="_blank" rel="noopener">https://blog.csdn.net/laolu1573/article/details/77170407</a><br>版权声明：本文为博主原创文章，转载请附上博文链接！</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer阅读</title>
      <link href="/2019/03/11/Transformer-master%E9%98%85%E8%AF%BB/"/>
      <url>/2019/03/11/Transformer-master%E9%98%85%E8%AF%BB/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p><p><a href="https://blog.csdn.net/mijiaoxiaosan/article/details/74909076" target="_blank" rel="noopener">机器翻译模型Transformer代码详细解析</a></p><p><a href="https://github.com/Xinghanzzy/transformer-simple/" target="_blank" rel="noopener">代码地址</a></p></blockquote><h1 id="Transformer"><a href="#Transformer" class="headerlink" title="Transformer"></a>Transformer</h1><p>动机：前几天和同学讨论一下decoder对mask是否有优化，发现自己对tensor shape记得并不好，这两天再过一次 加深一下印象并记录</p><p>代码：17年老代码，好处是写的很简单</p><h2 id="数据预处理-prepro"><a href="#数据预处理-prepro" class="headerlink" title="数据预处理 prepro"></a>数据预处理 prepro</h2><ol><li>洗数据：去除非拉丁字符 </li><li>词频统计，$Counter()$，降序</li><li>PAD,UNK, S, /S，文件开头，数量1000000000</li></ol><h2 id="train"><a href="#train" class="headerlink" title="train"></a>train</h2><ul><li><p>加载词表，$2*2$个list，$lang2idx，idx2lang$</p></li><li><p>构图</p><ul><li><p>默认图</p></li><li><p>读取数据batch，输出为ont-hot idx</p><blockquote><p>input : none</p><p>output : (N, T)     </p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># N：batch_size  T:maxlen 最大句长</span></span><br><span class="line">self.x, self.y, self.num_batch = get_batch_data() <span class="comment"># (N, T) </span></span><br><span class="line">lang2idx，idx2lang</span><br><span class="line">word2idx: OOV-&gt;"&lt;UNK&gt;"(1) 句尾+&lt;/S&gt;</span><br><span class="line">    <span class="comment"># x和y后面pad上x和y初始元素个数和句子最大长度差的那么多数值 0</span></span><br><span class="line">    <span class="comment"># pad 垫，填补</span></span><br><span class="line">PAD:补全<span class="number">0</span>，maxlen</span><br><span class="line">    num_batch = len(X) // hp.batch_size <span class="comment"># 整除</span></span><br><span class="line">    <span class="comment"># Create Queues</span></span><br><span class="line">    input_queues = tf.train.slice_input_producer([X, Y])</span><br><span class="line">    x, y = tf.train.shuffle_batch</span><br></pre></td></tr></table></figure></li><li><p>decoder input : 句尾去掉\S句首+ S (2)</p><blockquote><p>input :  (N, T) </p><p>output : (N, T) </p><p>用self.y来初始化解码器的输入。decoder_inputs和self.y相比，去掉了最后一个句子结束符，而在每句话最前面加了一个初始化为2的id，即S</p><p> scr: 今天天气很好。&lt;/S&gt;<br> tgt:     It’s a beautiful day today .     &lt;/S&gt;<br> pred:    <s> It’s a   beautiful day today .<br>   “<s>“ predict “It’s” ,   “<s> It’s” predict “a” , “.” predict “</s>“</s></s></p></blockquote></li><li><p>$2*2$个list，$lang2idx，idx2lang$</p></li><li><p>$encoder$</p><blockquote><p>input :  (N, T) </p><p>output : (N, T, num_units)   # num_units:隐层大小</p></blockquote><ul><li><p>embedding</p><blockquote><p>input :  (N, T) </p><p>output : (N, T, num_units)\</p></blockquote><p>建立 $lookup_table$ (vocab_size, num_units)</p><p>zero_pad:第一行置0</p><p>tf.nn.embedding_lookup(lookup_table, inputs)</p><p>这里推测输出 inputs中idx对应的vector组成的tensor(N, T, num_units)</p><p>缩放：scale  $outputs * (\sqrt{ num_units })$</p></li><li><p>positional_encoding</p><p>sin cos 常量 或者 调用embedding</p></li><li><p>dropout</p><p>​    0，其他缩放1/(1-rate)</p><p>​    maybe 可训练更新</p></li><li><p>6层attention encoder self-attention</p><blockquote><p>input :  (N, T, num_units)   </p><p>output : (N, T, num_units)</p></blockquote><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">multihead_attention(queries=self.enc, keys=self.enc, num_units=hp.hidden_units, num_heads=hp.num_heads, dropout_rate=hp.dropout_rate, is_training=is_training, causality=<span class="literal">False</span>)</span><br><span class="line">    Q K V = dense num_units relu <span class="comment"># (N, T_q, C) (N, T_k, C) (N, T_k, C)</span></span><br><span class="line">    Q_ K_ V_ =最后一维拆head分，在第一维拼接cat <span class="comment"># (h*N, T_q, C/h) (h*N, T_k, C/h)  </span></span><br><span class="line">    Q_ matmul V_  <span class="comment"># (h*N, T_q, T_k)  矩阵乘法 Q 乘 k 转置</span></span><br><span class="line">    scale：outputs / (num_units)**<span class="number">0.5</span></span><br><span class="line">    Key Masking</span><br><span class="line">     生成矩阵，key中最后一维加<span class="number">0</span>的变为无穷小 <span class="number">-2</span>**<span class="number">32</span>+<span class="number">1</span> <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line">    <span class="comment"># causality参数告知我们是否屏蔽未来序列的信息</span></span><br><span class="line">        <span class="comment"># 首先定义一个和outputs后两维的shape相同shape（T_q,T_k）的一个张量（矩阵）。 </span></span><br><span class="line">        <span class="comment"># 然后将该矩阵转为三角阵tril。三角阵中，对于每一个T_q,</span></span><br><span class="line">        <span class="comment"># 凡是那些大于它角标的T_k值全都为</span></span><br><span class="line">        <span class="comment"># 0，这样作为mask就可以让query只取它之前的key（self attention中query即key）。</span></span><br><span class="line">        <span class="comment"># 由于该规律适用于所有query，接下来仍用tile扩展堆叠其第一个维度，构成masks，</span></span><br><span class="line">        <span class="comment"># shape为(h*N, T_q,T_k).</span></span><br><span class="line">    softmax</span><br><span class="line">    Query Masking <span class="comment"># (h*N, T_q, T_k)</span></span><br><span class="line">    dropout</span><br><span class="line">    outputs = tf.matmul(outputs, V_) <span class="comment"># ( h*N, T_q, C/h)</span></span><br><span class="line">    concat <span class="comment"># (N, T_q, C)</span></span><br><span class="line">    +querys <span class="comment"># residual</span></span><br><span class="line">    normalize <span class="comment"># 随训练会变</span></span><br><span class="line">feedforward()</span><br><span class="line">两层卷积之间加了relu非线性操作。</span><br><span class="line">    之后是residual操作加上inputs残差</span><br><span class="line">    然后是normalize</span><br></pre></td></tr></table></figure></li></ul></li><li><p>$decoder$</p><ul><li><p>word embedding </p></li><li><p>positional embedding </p></li><li><p>dropout  # (N, T_q, C)</p></li><li><p>6层attention decoder self-attention  encoder-decoder attention</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">self:</span><br><span class="line">causality参数为<span class="literal">True</span>，以屏蔽未来的信息</span><br><span class="line">e-d:</span><br><span class="line">query:decoder</span><br><span class="line">keys: encoder</span><br><span class="line"><span class="comment"># causality设置为False 解码器中的信息都可以被用到</span></span><br><span class="line">feedforward:</span><br></pre></td></tr></table></figure></li></ul></li></ul></li></ul><ul><li><p>准确率 acc</p></li><li><p>smooth</p><p>​    self.y转为one_hot之后用module中定义的label_smoothing函数进行平滑操作 </p><p>​    ((1-epsilon) * inputs) + (epsilon / K) ：0-&gt;很小的树 1-&gt;接近1的数</p></li><li><p>loss：</p><p>​     预测值 和 平滑后的lable 交叉熵</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> NLP </tag>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>FFN MLP dense 权重矩阵 全连接</title>
      <link href="/2019/03/11/FFN%20MLP%20dense%20%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5%20%E5%85%A8%E8%BF%9E%E6%8E%A5/"/>
      <url>/2019/03/11/FFN%20MLP%20dense%20%E6%9D%83%E9%87%8D%E7%9F%A9%E9%98%B5%20%E5%85%A8%E8%BF%9E%E6%8E%A5/</url>
      
        <content type="html"><![CDATA[<blockquote><p>参考文章，感谢作者付出。</p><p><a href="https://blog.csdn.net/blogshinelee/article/details/84826837/" target="_blank" rel="noopener">直观理解神经网络最后一层全连接+Softmax</a></p><p><a href="https://blog.csdn.net/zhq9695/article/details/84337984/" target="_blank" rel="noopener">花书+吴恩达深度学习（一）前馈神经网络（多层感知机 MLP）</a></p><p><a href="https://blog.csdn.net/qq_31713935/article/details/78784408/" target="_blank" rel="noopener">如何理解softmax </a></p><p><a href="https://blog.csdn.net/leviopku/article/details/83109422/" target="_blank" rel="noopener">【AI数学】Batch-Normalization详细解析</a></p><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p><p>一句话：FNN=MLP=n<em>dense=n</em>relu(Wx+b) </p></blockquote><h1 id="FeedForward"><a href="#FeedForward" class="headerlink" title="FeedForward"></a>FeedForward</h1><p>FNN FFN？傻傻分不清楚 </p><p><strong>前馈神经网络</strong>（feedforward neural network），又称作<strong>深度前馈网络</strong>（deep feedforward network）、<strong>多层感知机</strong>（multilayer perceptron，MLP） </p><p>每一个神经元由<strong>一个线性拟合</strong>和<strong>一个非线性激活函数</strong>组成 </p><p>不同层之间就是全连接</p><p><strong>我们的任务就是找到权值和偏置这些参数的值，使得输出的东西让我们满意，达到我们的要求。</strong> </p><p>我的理解是 输入经过一层（放大），然后在经过一层（缩小）</p><p>层的选择 $dense $还有 $conv1d$ 都有，和同学讨论，谷歌的transformer实现用的<strong>卷积</strong>，听说是卷积快一些。</p><h1 id="MLP"><a href="#MLP" class="headerlink" title="MLP"></a>MLP</h1><p>多层感知器（MLP，Multilayer Perceptron）是一种前馈人工神经网络模型 </p><p>也叫人工神经网络（ANN，Artificial Neural Network） </p><p>除了输入输出层，它中间可以有多个隐层，最简单的MLP只含一个隐层，即三层的结构 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g0zzty4jbuj207x03mwer.jpg" alt></p><p>多层感知机层与层之间是全连接的（全连接的意思就是：上一层的任何一个神经元与下一层的所有神经元都有连接） </p><h1 id="全连接层-dense"><a href="#全连接层-dense" class="headerlink" title="全连接层 dense"></a>全连接层 dense</h1><p>全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。 </p><p><strong>全连接层</strong>将<strong>权重矩阵</strong>与输入向量相乘再加上偏置 </p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g1001xjzuqj20b00bb74v.jpg" alt="一维全连接"></p><p>​                                          一维全连接</p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g10023khdej20dz0dpwfd.jpg" alt="二维全连接层 "></p><p>​                                        二维全连接层</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">dense(</span><br><span class="line">    inputs,</span><br><span class="line">    units,</span><br><span class="line">    activation=<span class="literal">None</span>,</span><br><span class="line">    use_bias=<span class="literal">True</span>,</span><br><span class="line">    kernel_initializer=<span class="literal">None</span>,</span><br><span class="line">    bias_initializer=tf.zeros_initializer(),</span><br><span class="line">    kernel_regularizer=<span class="literal">None</span>,</span><br><span class="line">    bias_regularizer=<span class="literal">None</span>,</span><br><span class="line">    activity_regularizer=<span class="literal">None</span>,</span><br><span class="line">    trainable=<span class="literal">True</span>,</span><br><span class="line">    name=<span class="literal">None</span>,</span><br><span class="line">    reuse=<span class="literal">None</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># inputs: 输入数据，2维tensor.</span></span><br><span class="line"><span class="comment"># units: 该层的神经单元结点数。</span></span><br><span class="line"><span class="comment"># activation: 激活函数.</span></span><br><span class="line"><span class="comment"># use_bias: Boolean型，是否使用偏置项.</span></span><br><span class="line"><span class="comment"># kernel_initializer: 卷积核的初始化器.</span></span><br><span class="line"><span class="comment"># bias_initializer: 偏置项的初始化器，默认初始化为0.</span></span><br><span class="line"><span class="comment"># kernel_regularizer: 卷积核化的正则化，可选.</span></span><br><span class="line"><span class="comment"># bias_regularizer: 偏置项的正则化，可选.</span></span><br><span class="line"><span class="comment"># activity_regularizer: 输出的正则化函数.</span></span><br><span class="line"><span class="comment"># trainable: Boolean型，表明该层的参数是否参与训练。如果为真则变量加入到图集合中GraphKeys.TRAINABLE_VARIABLES (see tf.Variable).</span></span><br><span class="line"><span class="comment"># name: 层的名字.</span></span><br><span class="line"><span class="comment"># reuse: Boolean型, 是否重复使用参数.</span></span><br><span class="line"><span class="comment"># 全连接层执行操作 outputs = activation(inputs.kernel + bias)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果执行结果不想进行激活操作，则设置activation=None。</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Softmax and Normalization</title>
      <link href="/2019/03/11/Softmax%20and%20Normalization/"/>
      <url>/2019/03/11/Softmax%20and%20Normalization/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://blog.csdn.net/red_stone1/article/details/80687921/" target="_blank" rel="noopener">三分钟带你对 Softmax 划重点</a></p><p><a href="https://www.zhihu.com/question/38102762/answer/85238569?utm_source=wechat_session&amp;utm_medium=social&amp;utm_oi=629421655403925504/" target="_blank" rel="noopener">深度学习中 Batch Normalization为什么效果好？-知乎</a></p><p><a href="https://zhuanlan.zhihu.com/p/33173246/" target="_blank" rel="noopener">详解深度学习中的Normalization，BN/LN/WN</a></p><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p></blockquote><h1 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h1><p>在数学，尤其是概率论和相关领域中，Softmax函数，或称归一化 指数函数，是逻辑函数的一种推广。</p><p>Softmax函数实际上是有限项离散概率分布的梯度对数归一化。</p><p>它能将一个含任意实数的K维向量  “压缩”到另一个K维实向量  中，使得每一个元素的范围都在$(0, 1)$之间，并且所有元素的和为$1$。</p><script type="math/tex; mode=display">S_i=\frac {e^{V_i}} {\sum_{i}^{c} {e^{V_i}}}</script><p>在实际应用中遇到提前$padding$时候将$V_i$设置为$-2^{32}+1$这种情况</p><p>为了是经过$Softmax$之后可以把值变为0</p><h1 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h1><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><h3 id="what"><a href="#what" class="headerlink" title="what"></a>what</h3><p>​    顾名思义，batch normalization嘛，就是“批规范化”咯。Google在ICML文中描述的非常清晰，即在每次SGD时，通过mini-batch来对相应的activation做规范化操作，<strong>使得结果（输出信号各个维度）的均值为0，方差为1.</strong> </p><p>​    最后的“scale and shift”操作则是为了让因训练所需而“刻意”加入的BN能够有可能还原最初的输入（即当<img src="https://www.zhihu.com/equation?tex=%5Cgamma%5E%7B%28k%29%7D%3D%5Csqrt%7BVar%5Bx%5E%7B%28k%29%7D%5D%7D%2C+%5Cbeta%5E%7B%28k%29%7D%3DE%5Bx%5E%7B%28k%29%7D%5D" alt="\gamma^{(k)}=\sqrt{Var[x^{(k)}]}, \beta^{(k)}=E[x^{(k)}]">   $\gamma^{(k)}=\sqrt{Var[x^{(k)}]}, \beta^{(k)}=E[x^{(k)}]$ 时候，相当于未做normalization），从而保证整个network的capacity（容量）。（有关capacity的解释：实际上BN可以看作是在原模型上加入的“新操作”，这个新操作很大可能会改变某层原来的输入。当然也可能不改变，不改变的时候就是“还原原来输入”。如此一来，既可以改变同时也可以保持原输入，那么模型的容纳能力（capacity）就提升了。）</p><p><img src="https://pic2.zhimg.com/9ad70be49c408d464c71b8e9a006d141_r.jpg" alt="preview"> </p><h3 id="where"><a href="#where" class="headerlink" title="where"></a>where</h3><p>​    BN可以应用于网络中任意的activation set。文中还特别指出在CNN中，BN应作用在非线性映射前，即对做规范化。 </p><h3 id="why"><a href="#why" class="headerlink" title="why"></a>why</h3><p>​    说到底，BN的提出还是为了克服深度神经网络难以训练的弊病。其实BN背后的insight非常简单，只是在文章中被Google复杂化了。</p><blockquote><p>首先来说说“Internal Covariate Shift”。文章的title除了BN这样一个关键词，还有一个便是“ICS”。</p><p>大家都知道在统计机器学习中的一个经典假设是“源空间（source domain）和目标空间（target domain）的数据分布（distribution）是一致的”。如果不一致，那么就出现了新的机器学习问题，如，transfer learning/domain adaptation等。</p><p>covariate shift就是分布不一致假设之下的一个分支问题，它是指源空间和目标空间的条件概率是一致的，但是其边缘概率不同，即：对所有<img src="https://www.zhihu.com/equation?tex=x%5Cin+%5Cmathcal%7BX%7D" alt="x\in \mathcal{X}">,<img src="https://www.zhihu.com/equation?tex=P_s%28Y%7CX%3Dx%29%3DP_t%28Y%7CX%3Dx%29" alt="P_s(Y|X=x)=P_t(Y|X=x)">，但是<img src="https://www.zhihu.com/equation?tex=P_s%28X%29%5Cne+P_t%28X%29" alt="P_s(X)\ne P_t(X)">. 大家细想便会发现，的确，对于神经网络的各层输出，由于它们经过了层内操作作用，其分布显然与各层对应的输入信号分布不同，而且差异会随着网络深度增大而增大，可是它们所能“指示”的样本标记（label）仍然是不变的，这便符合了covariate shift的定义。由于是对层间信号的分析，也即是“internal”的来由。</p></blockquote><p>那BN到底是什么原理呢？说到底还是<strong>为了防止“梯度弥散”</strong>。关于梯度弥散，大家都知道一个简单的栗子：<img src="https://www.zhihu.com/equation?tex=0.9%5E%7B30%7D%5Capprox+0.04" alt="0.9^{30}\approx 0.04">。在BN中，是通过将activation规范为均值和方差一致的手段使得原本会减小的activation的scale变大。可以说是一种更有效的local response normalization方法（见4.2.1节）。</p><p>梯度弥散、梯度爆炸</p><blockquote><p>靠近输出层的hidden layer 梯度大，参数更新快，所以很快就会收敛；</p><p>而靠近输入层的hidden layer 梯度小，参数更新慢，几乎就和初始状态一样，随机分布。</p><p>在上面的四层隐藏层网络结构中，第一层比第四层慢了接近100倍！！</p><p>这种现象就是梯度弥散（vanishing gradient problem）。而在另一种情况中，前面layer的梯度通过训练变大，而后面layer的梯度指数级增大，这种现象又叫做梯度爆炸(exploding gradient problem)。</p><p>总的来说，就是在这个深度网络中，梯度相当不稳定(unstable)。</p></blockquote><script type="math/tex; mode=display">\gamma^{(k)}=\sqrt{Var[x^{(k)}]}, \beta^{(k)}=E[x^{(k)}]</script><p><strong>Batch Normalization —— 纵向规范化</strong> </p><p><strong>Layer Normalization —— 横向规范化</strong> </p><blockquote><p>原来的网络，如果简单地增加深度，会导致梯度弥散或梯度爆炸。对于该问题的解决方法是正则化初始化和中间的正则化层（Batch Normalization），这样的话可以训练几十层的网络。</p><p>虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。</p><p>residual</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> TensorFlow </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Q K V 理解</title>
      <link href="/2019/03/08/Q%20K%20V%E7%90%86%E8%A7%A3/"/>
      <url>/2019/03/08/Q%20K%20V%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<blockquote><p>正所谓前人栽树，后人乘凉。</p><p>感谢<a href="http://blog.csdn.net/qq_41058526/article/details/80578932" target="_blank" rel="noopener"> seeInfinite：关于attention机制在nlp中的应用总结</a></p><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p></blockquote><h1 id="Query-Key-Value"><a href="#Query-Key-Value" class="headerlink" title="Query Key Value"></a>Query Key Value</h1><h1 id="概念理解"><a href="#概念理解" class="headerlink" title="概念理解"></a>概念理解</h1><h3 id="单词本意"><a href="#单词本意" class="headerlink" title="单词本意"></a>单词本意</h3><p>query：n. 疑问，质问；疑问号 ；[计] 查询 vt. 询问；对……表示疑问</p><p>key：vt. 键入；锁上；调节…的音调；提供线索</p><p>value：n. 值；价值；价格；重要性；确切涵义</p><h3 id="计算机释义"><a href="#计算机释义" class="headerlink" title="计算机释义"></a>计算机释义</h3><p>Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射</p><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g0vhrmxn0xj20p50azq43.jpg" alt="attention中Q K V"></p><p>在计算attention时主要分为三步:</p><ol><li>第一步是将query和每个key进行相似度计算得到权重，常用的相似度函数有点积，拼接，感知机等；</li><li>第二步一般是使用一个softmax函数对这些权重进行归一化；</li><li>最后将权重和相应的键值value进行加权求和得到最后的attention。</li></ol><ul><li>目前在NLP研究中，key和value常常都是同一个，即key=value。</li></ul><h3 id="自己理解"><a href="#自己理解" class="headerlink" title="自己理解"></a>自己理解</h3><p><strong>encoder-decoder的attention计算的Q、K、V就是</strong> </p><p>​    Q:encoder信息</p><p>​    K、V：decoder信息</p><p><strong>self-attention中：</strong></p><p>Q = K = V</p><ul><li><p>先乘再拆    1：</p><p> 函数输入为上一层的输出a，a与一个权重矩阵相乘，拆分为三份既是Q K V</p></li><li><p>卷积做法：</p> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Input Embedding: Batch * Length * Hidden</span><br><span class="line"></span><br><span class="line">Posional Encoding : （Batch *）Length * Hidden</span><br><span class="line"></span><br><span class="line">Encoder Input = Input Embedding + Posional Encoding</span><br><span class="line"></span><br><span class="line">Query,Key,Value = Conv(Encoder Input,Hidden,3*Hidden)</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>过去做法：</p><p>函数输入为上一层的输出a，三个一样的，dense relu ，然后split</p></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> NMT </tag>
            
            <tag> NLP </tag>
            
            <tag> 概念理解 </tag>
            
            <tag> transformer </tag>
            
            <tag> attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Subword BPE 理解</title>
      <link href="/2019/03/08/Subword%20BPE%20%E7%90%86%E8%A7%A3&amp;sentence%20piece/"/>
      <url>/2019/03/08/Subword%20BPE%20%E7%90%86%E8%A7%A3&amp;sentence%20piece/</url>
      
        <content type="html"><![CDATA[<blockquote><p>正所谓前人栽树，后人乘凉。</p><p>感谢<a href="http://blog.csdn.net/u013453936/article/details/80878412" target="_blank" rel="noopener">夏天的米米阳光CSDN</a></p><p>感谢<a href="https://www.jianshu.com/p/9b93cef1ca56" target="_blank" rel="noopener">自然语言处理之_SentencePiece分词</a></p><p>感谢<a href="http://plmsmile.github.io/2017/10/19/subword-units/" target="_blank" rel="noopener">subword-units</a></p><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p></blockquote><h1 id="Subword"><a href="#Subword" class="headerlink" title="Subword"></a>Subword</h1><p><strong>BPE的训练和解码范围都是一个词的范围。</strong> </p><h2 id="learn-BPE"><a href="#learn-BPE" class="headerlink" title="learn BPE"></a>learn BPE</h2><p>BPE词表学习，首先统计词表词频，然后每个单词表示为一个字符序列，并加上一个特殊的词尾标记 </p><blockquote><p> apple eat 这两个的e是不同的</p><p>appale    e&lt;\w&gt;</p><p>eat        e</p></blockquote><p>取出频率最高的‘a b’加入词表中，并将‘a b’替换为‘ab’,重复过程</p><p>codec文件中保存的就是训练过程的字符对，文件中最开始的是训练时最先保存的字符，即具有较高的优先级。 </p><h3 id="例子"><a href="#例子" class="headerlink" title="例子"></a>例子</h3><p>the、and、$date</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#version: 0.2 </span><br><span class="line">t h</span><br><span class="line">i n</span><br><span class="line">a n</span><br><span class="line">th e&lt;/w&gt;</span><br><span class="line">t i</span><br><span class="line">r e</span><br><span class="line">e n</span><br><span class="line">an d&lt;/w&gt;</span><br><span class="line"></span><br><span class="line">d ate&lt;/w&gt;</span><br><span class="line">it s&lt;/w&gt;</span><br><span class="line">er e&lt;/w&gt;</span><br><span class="line">t a</span><br><span class="line">o g</span><br><span class="line">d s&lt;/w&gt;</span><br><span class="line">ent s&lt;/w&gt;</span><br><span class="line">ro m&lt;/w&gt;</span><br><span class="line">f rom&lt;/w&gt;</span><br><span class="line">ig h</span><br><span class="line">committe e&lt;/w&gt;</span><br><span class="line">on e&lt;/w&gt;</span><br><span class="line">st ate&lt;/w&gt;</span><br><span class="line">i r&lt;/w&gt;</span><br><span class="line">the ir&lt;/w&gt;</span><br><span class="line">a y&lt;/w&gt;</span><br><span class="line">$ date&lt;/w&gt;</span><br></pre></td></tr></table></figure><h2 id="apply-bpe"><a href="#apply-bpe" class="headerlink" title="apply bpe"></a>apply bpe</h2><p>按在词的范围中进行编码的，首先将词拆成一个一个的字符，然后按照训练得到的codec文件中的字符对来合并。 </p><h2 id="论文代码简单实现"><a href="#论文代码简单实现" class="headerlink" title="论文代码简单实现"></a>论文代码简单实现</h2><p>(subword-units 实现)</p><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">process_raw_words</span><span class="params">(words, endtag=<span class="string">'-'</span>)</span>:</span></span><br><span class="line">    <span class="string">'''把单词分割成最小的符号，并且加上结尾符号'''</span></span><br><span class="line">    vocabs = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> words.items():</span><br><span class="line">        <span class="comment"># 加上空格</span></span><br><span class="line">        word = re.sub(<span class="string">r'([a-zA-Z])'</span>, <span class="string">r' \1'</span>, word)</span><br><span class="line">        word += <span class="string">' '</span> + endtag</span><br><span class="line">        vocabs[word] = count</span><br><span class="line">    <span class="keyword">return</span> vocabs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_symbol_pairs</span><span class="params">(vocabs)</span>:</span></span><br><span class="line">    <span class="string">''' 获得词汇中所有的字符pair，连续长度为2，并统计出现次数</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        vocabs: 单词dict，(word, count)单词的出现次数。单词已经分割为最小的字符</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        pairs: ((符号1, 符号2), count)</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment">#pairs = collections.defaultdict(int)</span></span><br><span class="line">    pairs = dict()</span><br><span class="line">    <span class="keyword">for</span> word, freq <span class="keyword">in</span> vocabs.items():</span><br><span class="line">        <span class="comment"># 单词里的符号</span></span><br><span class="line">        symbols = word.split()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(symbols) - <span class="number">1</span>):</span><br><span class="line">            p = (symbols[i], symbols[i + <span class="number">1</span>])</span><br><span class="line">            pairs[p] = pairs.get(p, <span class="number">0</span>) + freq</span><br><span class="line">    <span class="keyword">return</span> pairs</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_symbols</span><span class="params">(symbol_pair, vocabs)</span>:</span></span><br><span class="line">    <span class="string">'''把vocabs中的所有单词中的'a b'字符串用'ab'替换</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        symbol_pair: (a, b) 两个符号</span></span><br><span class="line"><span class="string">        vocabs: 用subword(symbol)表示的单词，(word, count)。其中word使用subword空格分割</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        vocabs_new: 替换'a b'为'ab'的新词汇表</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    vocabs_new = &#123;&#125;</span><br><span class="line">    raw = <span class="string">' '</span>.join(symbol_pair)</span><br><span class="line">    merged = <span class="string">''</span>.join(symbol_pair)</span><br><span class="line">    <span class="comment"># 非字母和数字字符做转义</span></span><br><span class="line">    bigram =  re.escape(raw)</span><br><span class="line">    p = re.compile(<span class="string">r'(?&lt;!\S)'</span> + bigram + <span class="string">r'(?!\S)'</span>)</span><br><span class="line">    <span class="keyword">for</span> word, count <span class="keyword">in</span> vocabs.items():</span><br><span class="line">        word_new = p.sub(merged, word)</span><br><span class="line">        vocabs_new[word_new] = count</span><br><span class="line">    <span class="keyword">return</span> vocabs_new</span><br><span class="line"></span><br><span class="line">raw_words = &#123;<span class="string">"low"</span>:<span class="number">5</span>, <span class="string">"lower"</span>:<span class="number">2</span>, <span class="string">"newest"</span>:<span class="number">6</span>, <span class="string">"widest"</span>:<span class="number">3</span>&#125;</span><br><span class="line">vocabs = process_raw_words(raw_words)</span><br><span class="line"><span class="comment"># print(vocabs)</span></span><br><span class="line"></span><br><span class="line">num_merges = <span class="number">10</span></span><br><span class="line"><span class="keyword">print</span> (vocabs)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_merges):</span><br><span class="line">    pairs = get_symbol_pairs(vocabs)</span><br><span class="line">    <span class="comment"># 选择出现频率最高的pair</span></span><br><span class="line">    symbol_pair = max(pairs, key=pairs.get)</span><br><span class="line">    print(pairs)</span><br><span class="line">    print(symbol_pair)</span><br><span class="line">    vocabs = merge_symbols(symbol_pair, vocabs)</span><br><span class="line"><span class="keyword">print</span> (vocabs)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&apos; l o w -&apos;: 5, &apos; l o w e r -&apos;: 2, &apos; n e w e s t -&apos;: 6, &apos; w i d e s t -&apos;: 3&#125;</span><br><span class="line">&#123;(&apos;l&apos;, &apos;o&apos;): 7, (&apos;o&apos;, &apos;w&apos;): 7, (&apos;w&apos;, &apos;-&apos;): 5, (&apos;w&apos;, &apos;e&apos;): 8, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;e&apos;, &apos;s&apos;): 9, (&apos;s&apos;, &apos;t&apos;): 9, (&apos;t&apos;, &apos;-&apos;): 9, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;e&apos;): 3&#125;</span><br><span class="line">(&apos;e&apos;, &apos;s&apos;)</span><br><span class="line">&#123;(&apos;l&apos;, &apos;o&apos;): 7, (&apos;o&apos;, &apos;w&apos;): 7, (&apos;w&apos;, &apos;-&apos;): 5, (&apos;w&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;es&apos;): 6, (&apos;es&apos;, &apos;t&apos;): 9, (&apos;t&apos;, &apos;-&apos;): 9, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;es&apos;): 3&#125;</span><br><span class="line">(&apos;es&apos;, &apos;t&apos;)</span><br><span class="line">&#123;(&apos;l&apos;, &apos;o&apos;): 7, (&apos;o&apos;, &apos;w&apos;): 7, (&apos;w&apos;, &apos;-&apos;): 5, (&apos;w&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;est&apos;): 6, (&apos;est&apos;, &apos;-&apos;): 9, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est&apos;): 3&#125;</span><br><span class="line">(&apos;est&apos;, &apos;-&apos;)</span><br><span class="line">&#123;(&apos;l&apos;, &apos;o&apos;): 7, (&apos;o&apos;, &apos;w&apos;): 7, (&apos;w&apos;, &apos;-&apos;): 5, (&apos;w&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;est-&apos;): 6, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;l&apos;, &apos;o&apos;)</span><br><span class="line">&#123;(&apos;lo&apos;, &apos;w&apos;): 7, (&apos;w&apos;, &apos;-&apos;): 5, (&apos;w&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;est-&apos;): 6, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;lo&apos;, &apos;w&apos;)</span><br><span class="line">&#123;(&apos;low&apos;, &apos;-&apos;): 5, (&apos;low&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;n&apos;, &apos;e&apos;): 6, (&apos;e&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;est-&apos;): 6, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;n&apos;, &apos;e&apos;)</span><br><span class="line">&#123;(&apos;low&apos;, &apos;-&apos;): 5, (&apos;low&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;ne&apos;, &apos;w&apos;): 6, (&apos;w&apos;, &apos;est-&apos;): 6, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;ne&apos;, &apos;w&apos;)</span><br><span class="line">&#123;(&apos;low&apos;, &apos;-&apos;): 5, (&apos;low&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;new&apos;, &apos;est-&apos;): 6, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;new&apos;, &apos;est-&apos;)</span><br><span class="line">&#123;(&apos;low&apos;, &apos;-&apos;): 5, (&apos;low&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;low&apos;, &apos;-&apos;)</span><br><span class="line">&#123;(&apos;low&apos;, &apos;e&apos;): 2, (&apos;e&apos;, &apos;r&apos;): 2, (&apos;r&apos;, &apos;-&apos;): 2, (&apos;w&apos;, &apos;i&apos;): 3, (&apos;i&apos;, &apos;d&apos;): 3, (&apos;d&apos;, &apos;est-&apos;): 3&#125;</span><br><span class="line">(&apos;w&apos;, &apos;i&apos;)</span><br><span class="line">&#123;&apos; low-&apos;: 5, &apos; low e r -&apos;: 2, &apos; newest-&apos;: 6, &apos; wi d est-&apos;: 3&#125;</span><br></pre></td></tr></table></figure><h1 id="Sentence-piece"><a href="#Sentence-piece" class="headerlink" title="Sentence piece"></a>Sentence piece</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>​    SentencePiece是一个google开源的自然语言处理工具包。网上是这么描述它的：数据驱动、跨语言、高性能、轻量级——面向神经网络文本生成系统的无监督文本词条化工具。 </p><p>​    一些组合常常出现，但事先并不知道，于是我们想让机器自动学习经常组合出现的短语和词。SentencePiece就是来解决这个问题的。它需要大量文本来训练。</p><p> SentencePiece的用途不限于自然语言处理，记得DC之前有一个药物分子筛选的比赛，蛋白质的一级结构是氨基酸序列，需要研究氨基酸序列片断，片断的长度又是不固定的，此处就可以用SentencePiece进行切分。原理是重复出现次数多的片断，就认为是一个意群（词）。(<strong>未经过验证</strong>)</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><p> SentencePiece分为两部分：训练模型和使用模型，训练模型部分是用C语言实现的，可编成二进程程序执行，训练结果是生成一个model和一个词典文件。</p><p> 模型使用部分同时支持二进制程序和Python调用两种方式，训练完生成的词典数据是明文，可编辑，因此也可以用任何语言读取和使用。</p><h4 id="1-在Ubuntu系统中安装Python支持"><a href="#1-在Ubuntu系统中安装Python支持" class="headerlink" title="1) 在Ubuntu系统中安装Python支持"></a>1) 在Ubuntu系统中安装Python支持</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> sudo pip install SentencePiece</span><br></pre></td></tr></table></figure><h4 id="2-下载源码"><a href="#2-下载源码" class="headerlink" title="2) 下载源码"></a>2) 下载源码</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> git clone https://github.com/google/sentencepiece</span><br><span class="line"><span class="meta">$</span> cd sentencepiece</span><br><span class="line"><span class="meta">$</span> ./autogen.sh</span><br><span class="line"><span class="meta">$</span> ./confiture; make; sudo make install # 注意需要先安装autogen,automake等编译工具</span><br></pre></td></tr></table></figure><h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> spm_train --input=/tmp/a.txt --model_prefix=/tmp/test</span><br><span class="line"><span class="meta">#</span> --input指定需要训练的文本文件，--model_prefix指定训练好的模型名，本例中生成/tmp/test.model和/tmp/test.vocab两个文件，vocab是词典信息。</span><br><span class="line"><span class="meta">$</span> spm_train --input=&lt;input&gt; --model_prefix=&lt;model_name&gt; --vocab_size=8000 --model_type=&lt;type&gt;</span><br></pre></td></tr></table></figure><p>model_name为保存的模型为model_name.model,词典为model_name.vocab,词典大小可以人为设定vocab  _size.训练模型包括<code>unigram</code> (default), <code>bpe</code>, <code>char</code>, or <code>word</code>四种类型. </p><h2 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h2><h4 id="1-命令行调用"><a href="#1-命令行调用" class="headerlink" title="(1) 命令行调用"></a>(1) 命令行调用</h4><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">$</span> echo "食材上不会有这样的纠结" | spm_encode --model=/tmp/test.model</span><br></pre></td></tr></table></figure><h4 id="2-Python程序调用"><a href="#2-Python程序调用" class="headerlink" title="(2) Python程序调用"></a>(2) Python程序调用</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sentencepiece <span class="keyword">as</span> spm</span><br><span class="line"></span><br><span class="line">sp = spm.SentencePieceProcessor()</span><br><span class="line">text = <span class="string">"食材上不会有这样的纠结"</span> </span><br><span class="line"></span><br><span class="line">sp.Load(<span class="string">"/tmp/test.model"</span>) </span><br><span class="line">print(sp.EncodeAsPieces(text))</span><br></pre></td></tr></table></figure><h2 id="使用技巧"><a href="#使用技巧" class="headerlink" title="使用技巧"></a>使用技巧</h2><p> 如果我们分析某个领域相关问题，可以用该领域的书籍和文档去训练模型。并不限于被分析的内容本身。训练数据越多，模型效果越好。更多参数及用法，请见git上的说明文件。</p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><h4 id="1-用法示例"><a href="#1-用法示例" class="headerlink" title="(1) 用法示例"></a>(1) 用法示例</h4><p><a href="https://pypi.org/project/sentencepiece/0.0.0/" target="_blank" rel="noopener">https://pypi.org/project/sentencepiece/0.0.0/</a></p><h4 id="2-训练示例"><a href="#2-训练示例" class="headerlink" title="(2) 训练示例"></a>(2) 训练示例</h4><p><a href="https://github.com/google/sentencepiece#train-sentencepiece-model" target="_blank" rel="noopener">https://github.com/google/sentencepiece#train-sentencepiece-model</a></p><h1 id="Word-piece"><a href="#Word-piece" class="headerlink" title="Word piece"></a>Word piece</h1><p>Todo</p>]]></content>
      
      
      
        <tags>
            
            <tag> Subword </tag>
            
            <tag> BPE </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Github 简明教程</title>
      <link href="/2019/02/13/Github%20%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/"/>
      <url>/2019/02/13/Github%20%E7%AE%80%E6%98%8E%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>正所谓前人栽树，后人乘凉。</p><p>感谢<a href="http://www.runoob.com/w3cnote/git-guide.html" target="_blank" rel="noopener">菜鸟教程</a></p><p><a href="https://xinghanzzy.github.io/">我的的博客</a></p></blockquote><h3 id="配置Git"><a href="#配置Git" class="headerlink" title="配置Git"></a>配置Git</h3><p>首先在本地创建ssh key；</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh-keygen -t rsa -C &quot;your_email@youremail.com&quot;</span><br></pre></td></tr></table></figure><p>后面的your_email@youremail.com改为你在github上注册的邮箱，之后会要求确认路径和输入密码，我们这使用默认的一路回车就行。成功的话会在~/下生成.ssh文件夹，进去，打开id_rsa.pub，复制里面的key。</p><p>回到github上，进入 Account Settings（账户配置），左边选择SSH Keys，Add SSH Key,title随便填，粘贴在你电脑上生成的key。</p><p><img src="http://www.runoob.com/wp-content/uploads/2014/05/github-account.jpg" alt></p><p>为了验证是否成功，在git bash下输入：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure><p>为了验证是否成功，在git bash下输入：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ ssh -T git@github.com</span><br></pre></td></tr></table></figure></p><p>如果是第一次的会提示是否continue，输入yes就会看到：You’ve successfully authenticated, but GitHub does not provide shell access 。这就表示已成功连上github。</p><p>接下来我们要做的就是把本地仓库传到github上去，在此之前还需要设置username和email，因为github每次commit都会记录他们。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git config --global user.name &quot;your name&quot;</span><br><span class="line">$ git config --global user.email &quot;your_email@youremail.com&quot;</span><br></pre></td></tr></table></figure></p><p>进入要上传的仓库，右键git bash，添加远程地址：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ git remote add origin git@github.com:yourName/yourRepo.git</span><br></pre></td></tr></table></figure></p><p>后面的yourName和yourRepo表示你再github的用户名和刚才新建的仓库，加完之后进入.git，打开config，这里会多出一个remote “origin”内容，这就是刚才添加的远程地址，也可以直接修改config来配置远程地址。</p><p>创建新文件夹，打开，然后执行 git init 以创建新的 git 仓库。</p><h3 id="检出仓库"><a href="#检出仓库" class="headerlink" title="检出仓库"></a>检出仓库</h3><p>执行如下命令以创建一个本地仓库的克隆版本：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone /path/to/repository</span><br></pre></td></tr></table></figure><p>如果是远端服务器上的仓库，你的命令会是这个样子：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone username@host:/path/to/repository</span><br></pre></td></tr></table></figure></p><h3 id="工作流"><a href="#工作流" class="headerlink" title="工作流"></a>工作流</h3><p>你的本地仓库由 git 维护的三棵”树”组成。第一个是你的 工作目录，它持有实际文件；第二个是 暂存区（Index），它像个缓存区域，临时保存你的改动；最后是 HEAD，它指向你最后一次提交的结果。</p><p>你可以提出更改（把它们添加到暂存区），使用如下命令：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add &lt;filename&gt;</span><br><span class="line">git add *</span><br></pre></td></tr></table></figure></p><p>这是 git 基本工作流程的第一步；使用如下命令以实际提交改动：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git commit -m &quot;代码提交信息&quot;</span><br></pre></td></tr></table></figure></p><p>现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。</p><p><img src="http://www.runoob.com/wp-content/uploads/2014/05/trees.png" alt></p><p>推送改动</p><p>你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin master</span><br></pre></td></tr></table></figure></p><p>可以把 master 换成你想要推送的任何分支。 </p><p>如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git remote add origin &lt;server&gt;</span><br></pre></td></tr></table></figure></p><p>如此你就能够将你的改动推送到所添加的服务器上去了。</p><h3 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h3><p>分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是”默认的”分支。在其他分支上进行开发，完成后再将它们合并到主分支上。</p><p>branches<br>创建一个叫做”feature_x”的分支，并切换过去：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -b feature_x</span><br></pre></td></tr></table></figure></p><p>切换回主分支：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout master</span><br></pre></td></tr></table></figure></p><p>再把新建的分支删掉：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git branch -d feature_x</span><br></pre></td></tr></table></figure></p><p>除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git push origin &lt;branch&gt;</span><br></pre></td></tr></table></figure></p><h3 id="更新与合并"><a href="#更新与合并" class="headerlink" title="更新与合并"></a>更新与合并</h3><p>要更新你的本地仓库至最新改动，执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git pull</span><br></pre></td></tr></table></figure></p><p>以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。<br>要合并其他分支到你的当前分支（例如 master），执行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git merge &lt;branch&gt;</span><br></pre></td></tr></table></figure></p><p>在这两种情况下，git 都会尝试去自动合并改动。遗憾的是，这可能并非每次都成功，并可能出现冲突（conflicts）。 这时候就需要你修改这些文件来手动合并这些冲突（conflicts）。改完之后，你需要执行如下命令以将它们标记为合并成功：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add &lt;filename&gt;</span><br></pre></td></tr></table></figure></p><p>在合并改动之前，你可以使用如下命令预览差异：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git diff &lt;source_branch&gt; &lt;target_branch&gt;</span><br></pre></td></tr></table></figure></p><h3 id="标签"><a href="#标签" class="headerlink" title="标签"></a>标签</h3><p>为软件发布创建标签是推荐的。这个概念早已存在，在 SVN 中也有。你可以执行如下命令创建一个叫做 1.0.0 的标签：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git tag 1.0.0 1b2e1d63ff</span><br></pre></td></tr></table></figure></p><p>1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。可以使用下列命令获取提交 ID：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git log</span><br></pre></td></tr></table></figure></p><p>你也可以使用少一点的提交 ID 前几位，只要它的指向具有唯一性。</p><h3 id="替换本地改动"><a href="#替换本地改动" class="headerlink" title="替换本地改动"></a>替换本地改动</h3><p>假如你操作失误（当然，这最好永远不要发生），你可以使用如下命令替换掉本地改动：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git checkout -- &lt;filename&gt;</span><br></pre></td></tr></table></figure></p><p>此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到暂存区的改动以及新文件都不会受到影响。</p><p>假如你想丢弃你在本地的所有改动与提交，可以到服务器上获取最新的版本历史，并将你本地主分支指向它：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git fetch origin</span><br><span class="line">git reset --hard origin/master</span><br></pre></td></tr></table></figure></p><h3 id="实用小贴士"><a href="#实用小贴士" class="headerlink" title="实用小贴士"></a>实用小贴士</h3><p>内建的图形化 git：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gitk</span><br></pre></td></tr></table></figure></p><p>彩色的 git 输出：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config color.ui true</span><br></pre></td></tr></table></figure></p><p>显示历史记录时，每个提交的信息只显示一行：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git config format.pretty oneline</span><br></pre></td></tr></table></figure></p><p>交互式添加文件到暂存区：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git add -i</span><br></pre></td></tr></table></figure></p>]]></content>
      
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>测试</title>
      <link href="/2019/02/07/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A22/"/>
      <url>/2019/02/07/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A22/</url>
      
        <content type="html"><![CDATA[<h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><p>第一行</p><blockquote><p>123</p></blockquote>]]></content>
      
      
      
        <tags>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>使用 .gitignore 忽略 Git 仓库中的文件</title>
      <link href="/2017/02/22/%E4%BD%BF%E7%94%A8-.gitignore-%E5%BF%BD%E7%95%A5-git-%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6/"/>
      <url>/2017/02/22/%E4%BD%BF%E7%94%A8-.gitignore-%E5%BF%BD%E7%95%A5-git-%E4%BB%93%E5%BA%93%E4%B8%AD%E7%9A%84%E6%96%87%E4%BB%B6/</url>
      
        <content type="html"><![CDATA[<blockquote><p>使用 <code>.gitignore</code> 文件忽略指定文件</p></blockquote><h2 id="gitignore"><a href="#gitignore" class="headerlink" title=".gitignore"></a>.gitignore</h2><p>在Git中，很多时候你只想将代码提交到仓库，而不是将当前文件目录下的文件全部提交到Git仓库中，例如在MacOS系统下面的<code>.DS_Store</code>文件，或者是Xocde的操作记录，又或者是pod库的中一大串的源代码。这种情况下使用<code>.gitignore</code>就能够在Git提交时自动忽略掉这些文件。</p><h2 id="忽略的格式"><a href="#忽略的格式" class="headerlink" title="忽略的格式"></a>忽略的格式</h2><ul><li><code>#</code> :此为注释 – 将被 Git 忽略</li><li><code>*.a</code> :忽略所有 <code>.a</code> 结尾的文件</li><li><code>!lib.a</code> : 不忽略 <code>lib.a</code> 文件</li><li><code>/TODO</code> :仅仅忽略项目根目录下的 <code>TODO</code> 文件,不包括 <code>subdir/TODO</code></li><li><code>build/</code> : 忽略 <code>build/</code> 目录下的所有文件</li><li><code>doc/*.txt</code> : 会忽略 <code>doc/notes.txt</code> 但不包括 <code>doc/server/arch.txt</code></li></ul><h2 id="创建方法"><a href="#创建方法" class="headerlink" title="创建方法"></a>创建方法</h2><h4 id="从-github-上获取"><a href="#从-github-上获取" class="headerlink" title="从 github 上获取"></a>从 <a href="https://github.com/github/gitignore.git" target="_blank" rel="noopener">github</a> 上获取</h4><p>github上整理了一些常用需要的项目中需要忽略的文件配置，根据需要进行获取</p><pre><code>https://github.com/github/gitignore.git</code></pre><p>与 Xcode 相关的三个文件</p><ul><li>Xcode.gitignore</li><li>Objective-C.gitignore</li><li>Swift.gitignore</li></ul><p><code>Xcode.gitignore</code>忽略 <code>Xcode</code> 配置信息，如操作记录，默认打开窗口等</p><p>其他两个在 <code>Xcode.gitignore</code> 基础上针对不同的语言进行忽略</p><p>将这些文件重写命名为 <code>.gittignore</code></p><pre><code>$ mv Swift.gitignore .gittignore</code></pre><h4 id="通过-gitignore-io-创建（推荐）"><a href="#通过-gitignore-io-创建（推荐）" class="headerlink" title="通过 gitignore.io 创建（推荐）"></a>通过 <a href="https://www.gitignore.io/" target="_blank" rel="noopener">gitignore.io</a> 创建（推荐）</h4><h6 id="先自定义终端命令："><a href="#先自定义终端命令：" class="headerlink" title="先自定义终端命令："></a>先自定义终端命令：</h6><p>macOS下默认是<code>\#!/bin/bash</code>：</p><pre><code>$ echo &quot;function gi() { curl -L -s https://www.gitignore.io/api/\$@ ;}&quot; &gt;&gt; ~/.bash_profile &amp;&amp; source ~/.bash_profile</code></pre><p>如果是 <code>#!/bin/zsh</code></p><pre><code>$ echo &quot;function gi() { curl -L -s https://www.gitignore.io/api/\$@ ;}&quot; &gt;&gt; ~/.zshrc &amp;&amp; source ~/.zshrc</code></pre><h6 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h6><p>在当前终端目录下</p><pre><code>$ gi swift &gt; .gitignore</code></pre><p>就会针对 Swifit 类型的工程创建 <code>.gitignore</code> 文件。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 终端 </tag>
            
            <tag> Git </tag>
            
            <tag> Github </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git 代码回滚</title>
      <link href="/2017/02/16/Git-%E4%BB%A3%E7%A0%81%E5%9B%9E%E6%BB%9A/"/>
      <url>/2017/02/16/Git-%E4%BB%A3%E7%A0%81%E5%9B%9E%E6%BB%9A/</url>
      
        <content type="html"><![CDATA[<blockquote><p>并不适合阅读的个人文档。</p></blockquote><h1 id="git-revert-和-git-reset-的区别"><a href="#git-revert-和-git-reset-的区别" class="headerlink" title="git revert 和 git reset 的区别"></a><strong>git revert</strong> 和 <strong>git reset</strong> 的区别</h1><p> 先看图：</p><p><img src="https://ww3.sinaimg.cn/large/006tNbRwgy1fcr9tu6vdjj30t30ez0y8.jpg" alt></p><p><strong>sourceTree</strong> 中 <strong>revert</strong> 译为<strong><code>提交回滚</code></strong>，作用为忽略你指定的版本，然后提交一个新的版本。新的版本中已近删除了你所指定的版本。</p><p><strong>reset</strong> 为 <strong>重置到这次提交</strong>，将内容重置到指定的版本。<code>git reset</code> 命令后面是需要加2种参数的：<code>–-hard</code> 和 <code>–-soft</code>。这条命令默认情况下是 <code>-–soft</code>。</p><p>执行上述命令时，这该条commit号之 后（时间作为参考点）的所有commit的修改都会退回到git缓冲区中。使用<code>git status</code> 命令可以在缓冲区中看到这些修改。而如果加上<code>-–hard</code>参数，则缓冲区中不会存储这些修改，git会直接丢弃这部分内容。可以使用 <code>git push origin HEAD --force</code> 强制将分区内容推送到远程服务器。</p><h4 id="代码回退"><a href="#代码回退" class="headerlink" title="代码回退"></a>代码回退</h4><p>默认参数 <code>-soft</code>,所有commit的修改都会退回到git缓冲区<br>参数<code>--hard</code>，所有commit的修改直接丢弃</p><pre><code>$ git reset --hard HEAD^         回退到上个版本$ git reset --hard commit_id    退到/进到 指定commit_id</code></pre><p>推送到远程    </p><pre><code>$ git push origin HEAD --force</code></pre><h4 id="可以吃的后悔药-gt-版本穿梭"><a href="#可以吃的后悔药-gt-版本穿梭" class="headerlink" title="可以吃的后悔药-&gt;版本穿梭"></a>可以吃的后悔药-&gt;版本穿梭</h4><p>当你回滚之后，又后悔了，想恢复到新的版本怎么办？</p><p>用<code>git reflog</code>打印你记录你的每一次操作记录</p><pre><code>$ git reflog输出：c7edbfe HEAD@{0}: reset: moving to c7edbfefab1bdbef6cb60d2a7bb97aa80f022687470e9c2 HEAD@{1}: reset: moving to 470e9c2b45959e HEAD@{2}: revert: Revert &quot;add img&quot;470e9c2 HEAD@{3}: reset: moving to 470e9c22c26183 HEAD@{4}: reset: moving to 2c261830f67bb7 HEAD@{5}: revert: Revert &quot;add img&quot;</code></pre><p>找到你操作的id如：<code>b45959e</code>，就可以回退到这个版本</p><pre><code>$ git reset --hard b45959e</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 终端 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Git指令整理</title>
      <link href="/2017/02/15/Git%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/"/>
      <url>/2017/02/15/Git%E6%8C%87%E4%BB%A4%E6%95%B4%E7%90%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>随便整理的一些自用的Git指令</p></blockquote><h1 id="GitHub创建仓库提示代码"><a href="#GitHub创建仓库提示代码" class="headerlink" title="GitHub创建仓库提示代码"></a>GitHub创建仓库提示代码</h1><pre><code>echo &quot;# 项目名&quot; &gt;&gt; README.mdgit initgit add README.mdgit commit -m &quot;first commit&quot;git remote add origin git@github.com:qiubaiying/项目名.gitgit push -u origin master</code></pre><p>若仓库存在直接push</p><pre><code>git remote add origin git@github.com:qiubaiying/test.gitgit push -u origin master</code></pre><h1 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h1><h4 id="创建仓库（初始化）"><a href="#创建仓库（初始化）" class="headerlink" title="创建仓库（初始化）"></a>创建仓库（初始化）</h4><pre><code>在当前指定目录下创建git init新建一个仓库目录git init [project-name]克隆一个远程项目git clone [url]</code></pre><h4 id="添加文件到缓存区"><a href="#添加文件到缓存区" class="headerlink" title="添加文件到缓存区"></a>添加文件到缓存区</h4><pre><code>添加所有变化的文件 git add .添加名称指定文件git add text.txt</code></pre><h4 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h4><pre><code>设置提交代码时的用户信息git config [--global] user.name &quot;[name]&quot;git config [--global] user.email &quot;[email address]&quot;</code></pre><h4 id="提交"><a href="#提交" class="headerlink" title="提交"></a>提交</h4><pre><code>提交暂存区到仓库区git commit -m &quot;msg&quot;# 提交暂存区的指定文件到仓库区$ git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区$ git commit -a# 提交时显示所有diff信息$ git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息$ git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化$ git commit --amend [file1] [file2] ...</code></pre><h4 id="远程同步"><a href="#远程同步" class="headerlink" title="远程同步"></a>远程同步</h4><pre><code># 下载远程仓库的所有变动$ git fetch [remote]# 显示所有远程仓库$ git remote -v# 显示某个远程仓库的信息$ git remote show [remote]# 增加一个新的远程仓库，并命名$ git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并$ git pull [remote] [branch]# 上传本地指定分支到远程仓库$ git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突$ git push [remote] --force# 推送所有分支到远程仓库$ git push [remote] --all</code></pre><h4 id="分支"><a href="#分支" class="headerlink" title="分支"></a>分支</h4><pre><code># 列出所有本地分支$ git branch# 列出所有远程分支$ git branch -r# 列出所有本地分支和远程分支$ git branch -a# 新建一个分支，但依然停留在当前分支$ git branch [branch-name]# 新建一个分支，并切换到该分支$ git checkout -b [branch]# 新建一个分支，指向指定commit$ git branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系$ git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区$ git checkout [branch-name]# 切换到上一个分支$ git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间$ git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支$ git merge [branch]# 选择一个commit，合并进当前分支$ git cherry-pick [commit]# 删除分支$ git branch -d [branch-name]# 删除远程分支$ git push origin --delete [branch-name]$ git branch -dr [remote/branch]</code></pre><h4 id="标签Tags"><a href="#标签Tags" class="headerlink" title="标签Tags"></a>标签Tags</h4><pre><code>添加标签 在当前commitgit tag -a v1.0 -m &#39;xxx&#39; 添加标签 在指定commitgit tag v1.0 [commit]查看git tag删除git tag -d V1.0删除远程taggit push origin :refs/tags/[tagName]推送git push origin --tags拉取git fetch origin tag V1.0新建一个分支，指向某个taggit checkout -b [branch] [tag]</code></pre><h4 id="查看信息"><a href="#查看信息" class="headerlink" title="查看信息"></a>查看信息</h4><pre><code># 显示有变更的文件$ git status# 显示当前分支的版本历史$ git log# 显示commit历史，以及每次commit发生变更的文件$ git log --stat# 搜索提交历史，根据关键词$ git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行$ git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其&quot;提交说明&quot;必须符合搜索条件$ git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名$ git log --follow [file]$ git whatchanged [file]# 显示指定文件相关的每一次diff$ git log -p [file]# 显示过去5次提交$ git log -5 --pretty --oneline# 显示所有提交过的用户，按提交次数排序$ git shortlog -sn# 显示指定文件是什么人在什么时间修改过$ git blame [file]# 显示暂存区和工作区的差异$ git diff# 显示暂存区和上一个commit的差异$ git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异$ git diff HEAD# 显示两次提交之间的差异$ git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码$ git diff --shortstat &quot;@{0 day ago}&quot;# 显示某次提交的元数据和内容变化$ git show [commit]# 显示某次提交发生变化的文件$ git show --name-only [commit]# 显示某次提交时，某个文件的内容$ git show [commit]:[filename]# 显示当前分支的最近几次提交$ git reflog</code></pre><h4 id="撤销"><a href="#撤销" class="headerlink" title="撤销"></a>撤销</h4><pre><code># 恢复暂存区的指定文件到工作区$ git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区$ git checkout [commit] [file]# 恢复暂存区的所有文件到工作区$ git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变$ git reset [file]# 重置暂存区与工作区，与上一次commit保持一致$ git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变$ git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致$ git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变$ git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支$ git revert [commit]# 暂时将未提交的变化移除，稍后再移入$ git stash$ git stash pop</code></pre><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><pre><code># 生成一个可供发布的压缩包$ git archives</code></pre>]]></content>
      
      
      
        <tags>
            
            <tag> 终端 </tag>
            
            <tag> Git </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>快速搭建个人博客</title>
      <link href="/2017/02/06/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
      <url>/2017/02/06/%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<blockquote><p>正所谓前人栽树，后人乘凉。</p><p>感谢<a href="https://github.com/huxpro" target="_blank" rel="noopener">Huxpro</a>提供的博客模板</p><p><a href="http://qiubaiying.top" target="_blank" rel="noopener">我的的博客</a></p></blockquote><h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>从 Jekyll 到 GitHub Pages 中间踩了许多坑，终于把我的个人博客<a href="http://qiubaiying.top" target="_blank" rel="noopener">BY Blog</a>搭建出来了。。。</p><p>本教程针对的是不懂技术又想搭建个人博客的小白，操作简单暴力且快速。当然懂技术那就更好了。</p><p>看看看博客的主页样式：</p><p><a href="http://qiubaiying.github.io/" target="_blank" rel="noopener"><img src="http://upload-images.jianshu.io/upload_images/2178672-51a2fe6fbe24d1cd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></a></p><p>在手机上的布局：</p><p><a href="http://qiubaiying.github.io/" target="_blank" rel="noopener"><img src="http://upload-images.jianshu.io/upload_images/2178672-d58bb45f9faedb70.jpg" alt></a></p><p>废话不多说了，开始进入正文。</p><h1 id="快速开始"><a href="#快速开始" class="headerlink" title="快速开始"></a>快速开始</h1><h3 id="从注册一个Github账号开始"><a href="#从注册一个Github账号开始" class="headerlink" title="从注册一个Github账号开始"></a>从注册一个Github账号开始</h3><p>我采用的搭建博客的方式是使用 <a href="https://pages.github.com/" target="_blank" rel="noopener">GitHub Pages</a> + <a href="http://jekyll.com.cn/" target="_blank" rel="noopener">jekyll</a> 的方式。</p><p>要使用 GitHub Pages，首先你要注册一个<a href="https://github.com/" target="_blank" rel="noopener">GitHub</a>账号，GitHub 是全球最大的同性交友网站(吐槽下程序员~)，你值得拥有。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-e65e5cda50f38cef.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h3 id="拉取我的博客模板"><a href="#拉取我的博客模板" class="headerlink" title="拉取我的博客模板"></a>拉取我的博客模板</h3><p>注册完成后搜索 <code>qiubaiying.github.io</code> 进入<a href="https://github.com/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener">我的仓库</a></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-1b234fb8549e58aa.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>点击右上角的 <strong>Fork</strong> 将我的仓库拉倒你的账号下</p><p>稍等一下，点击刷新，你会看到<strong>Fork</strong>了成功的页面</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-b2347768a1f2d993.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h3 id="修改仓库名"><a href="#修改仓库名" class="headerlink" title="修改仓库名"></a>修改仓库名</h3><p>点击<strong>settings</strong>进入设置</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-f47b7e4802de6a34.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p></p><p id="Rename"></p><br>修改仓库名为 <code>你的Github账号名.github.io</code>，然后 Rename<p></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-ca3d843e526cdd5b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>这时你在在浏览器中输入 <code>你的Github账号名.github.io</code> 例如:<code>baiyingqiu.github.io</code></p><p>你将会看到如下界面</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-96b5db55df9db422.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>说明已经成功一半了😀。。。当然，还需要修改博客的配置才能变成你的博客。</p><p>若是出现</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-cfd55a22902a9d2c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>则需要 <a href="#Rename">检查一下你的仓库名是否正确</a></p><h3 id="整个网站结构"><a href="#整个网站结构" class="headerlink" title="整个网站结构"></a>整个网站结构</h3><p>修改Blog前我们来看看Jekyll 网站的基础结构，当然我们的网站比这个复杂。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">├── _config.yml</span><br><span class="line">├── _drafts</span><br><span class="line">|   ├── begin-with-the-crazy-ideas.textile</span><br><span class="line">|   └── on-simplicity-in-technology.markdown</span><br><span class="line">├── _includes</span><br><span class="line">|   ├── footer.html</span><br><span class="line">|   └── header.html</span><br><span class="line">├── _layouts</span><br><span class="line">|   ├── default.html</span><br><span class="line">|   └── post.html</span><br><span class="line">├── _posts</span><br><span class="line">|   ├── 2007-10-29-why-every-programmer-should-play-nethack.textile</span><br><span class="line">|   └── 2009-04-26-barcamp-boston-4-roundup.textile</span><br><span class="line">├── _data</span><br><span class="line">|   └── members.yml</span><br><span class="line">├── _site</span><br><span class="line">├── img</span><br><span class="line">└── index.html</span><br></pre></td></tr></table></figure><p>很复杂看不懂是不是，不要紧，你只要记住其中几个OK了</p><ul><li><code>_config.yml</code> 全局配置文件<br>   <code>_posts</code>    放置博客文章的文件夹<br>   <code>img</code>    存放图片的文件夹</li></ul><p>其他的想继续深究可以<a href="http://jekyll.com.cn/docs/structure/" target="_blank" rel="noopener">看这里</a></p><h3 id="修改博客配置"><a href="#修改博客配置" class="headerlink" title="修改博客配置"></a>修改博客配置</h3><p>来到你的仓库，找到<code>_config.yml</code>文件,这是网站的全局配置文件。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-c23d4a5d67c88084.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>点击修改</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-b37268df7a7852ca.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>然后编辑<code>_config.yml</code>的内容</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0c8750f5a18dbe03.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>接下来我们来详细说说以下配置文件的内容：</p><h4 id="基础设置"><a href="#基础设置" class="headerlink" title="基础设置"></a>基础设置</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Site settings</span><br><span class="line">title: You Blog      #你博客的标题</span><br><span class="line">SEOTitle: 你的博客 | You Blog     #显示在浏览器上搜索的时候显示的标题</span><br><span class="line">header-img: img/post-bg-rwd.jpg  #显示在首页的背景图片</span><br><span class="line">email: You@gmail.com</span><br><span class="line">description: &quot;You Blog&quot;   #网站介绍</span><br><span class="line">keyword: &quot;BY, BY Blog, 柏荧的博客, qiubaiying, 邱柏荧, iOS, Apple, iPhone&quot; #关键词</span><br><span class="line">url: &quot;https://qiubaiying.github.io&quot;          # 这个就是填写你的博客地址</span><br><span class="line">baseurl: &quot;&quot;      # 这个我们不用填写</span><br></pre></td></tr></table></figure><h4 id="侧边栏"><a href="#侧边栏" class="headerlink" title="侧边栏"></a>侧边栏</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Sidebar settings</span><br><span class="line">sidebar: true                           # 是否开启侧边栏.</span><br><span class="line">sidebar-about-description: &quot;说点装逼的话。。。&quot;</span><br><span class="line">sidebar-avatar:/img/avatar-by.JPG      # 你的个人头像 这里你可以改成我在img文件夹中的两张备用照片 img/avatar-m 或 avatar-g</span><br></pre></td></tr></table></figure><h4 id="社交账号"><a href="#社交账号" class="headerlink" title="社交账号"></a>社交账号</h4><p>展示你的其他社交平台</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-ec775a22f76e2f40.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>在下面你的社交账号的用户名就可以了，若没有可不用填</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># SNS settings</span><br><span class="line">RSS: false</span><br><span class="line">weibo_username:     username</span><br><span class="line">zhihu_username:     username</span><br><span class="line">github_username:    username</span><br><span class="line">facebook_username:  username</span><br><span class="line">jianshu_username:jianshu_id</span><br></pre></td></tr></table></figure><p>新加入了<strong>简书</strong>，<code>jianshu_id</code> 在你打开你的简书主页后的地址如：<code>http://www.jianshu.com/u/e71990ada2fd</code>中，后面这一串数字：<code>e71990ada2fd</code></p><h4 id="评论系统"><a href="#评论系统" class="headerlink" title="评论系统"></a>评论系统</h4><p>博客中使用的是 <a href="https://disqus.com/" target="_blank" rel="noopener">Disqus</a> 评论系统，在 <a href="https://disqus.com/" target="_blank" rel="noopener">官网</a> 注册帐号后，按下面的步骤简单的配置即可：</p><p>进入 <a href="https://disqus.com/home/settings/profile/" target="_blank" rel="noopener">设置页面</a> 配置个人信息</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-904ecb30c536c73b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="配置 Disqus 个人信息"></p><p>找到 <strong>Username</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-19d1b9e7d2624bfb.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="Disqus Account"></p><p>这个 <strong>Username</strong>  就是我们 <code>_config.yml</code> 中 <code>disqus_username</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Disqus settings（https://disqus.com/）</span><br><span class="line">disqus_username: qiubaiying</span><br></pre></td></tr></table></figure><blockquote><p>很对人反映 Disqus 评论插件加载不出来，因为 Disqus 在国内加载缓慢，所以我新集成了 Gitalk 评论插件（感谢<a href="https://github.com/FeDemo" target="_blank" rel="noopener">@FeDemo</a>的推荐），喜欢折腾的朋友可以看这篇：<a href="http://qiubaiying.top/2017/12/19/%E4%B8%BA%E5%8D%9A%E5%AE%A2%E6%B7%BB%E5%8A%A0-Gitalk-%E8%AF%84%E8%AE%BA%E6%8F%92%E4%BB%B6/" target="_blank" rel="noopener">《为博客添加 Gitalk 评论插件》</a>。 我已经在<code>_config.yml</code> 配置就好了，只需要填写参数可以了。</p></blockquote><h4 id="网站统计"><a href="#网站统计" class="headerlink" title="网站统计"></a>网站统计</h4><p>集成了 <a href="http://tongji.baidu.com/web/welcome/login" target="_blank" rel="noopener">Baidu Analytics</a> 和 <a href="http://www.google.cn/analytics/" target="_blank" rel="noopener">Google Analytics</a>，到各个网站注册拿到track_id替换下面的就可以了</p><p>这是我的 Google Analytics</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-c36b895c53196fdb.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><strong>不要使用我的track_id</strong>😂。。。</p><p>若不想启用统计，直接删除或注释掉就可以了</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Analytics settings</span><br><span class="line"># Baidu Analytics</span><br><span class="line">ba_track_id: 83e259f69b37d02a4633a2b7d960139c</span><br><span class="line"></span><br><span class="line"># Google Analytics</span><br><span class="line">ga_track_id: &apos;UA-90855596-1&apos;            # Format: UA-xxxxxx-xx</span><br><span class="line">ga_domain: auto</span><br></pre></td></tr></table></figure><h4 id="好友"><a href="#好友" class="headerlink" title="好友"></a>好友</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">friends: [</span><br><span class="line">    &#123;</span><br><span class="line">        title: &quot;简书·BY&quot;,</span><br><span class="line">        href: &quot;http://www.jianshu.com/u/e71990ada2fd&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        title: &quot;Apple&quot;,</span><br><span class="line">        href: &quot;https://apple.com&quot;</span><br><span class="line">    &#125;,&#123;</span><br><span class="line">        title: &quot;Apple Developer&quot;,</span><br><span class="line">        href: &quot;https://developer.apple.com/&quot;</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h4 id="保存"><a href="#保存" class="headerlink" title="保存"></a>保存</h4><p>讲网页拉倒底部，点击 <code>Commit changes</code> 提交保存</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0781006b5d15d149.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>再次进入你的主页，</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-a49ee2975d524c93.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>恭喜你，你的个人博客搭建完成了😀。</p><h1 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h1><p>利用 Github网站 ，我们可以不用学习<a href="https://git-scm.com/" target="_blank" rel="noopener">git</a>，就可以轻松管理自己的博客</p><p>对于轻车熟路的程序猿来说，使用git管理会更加方便。。。</p><h2 id="创建"><a href="#创建" class="headerlink" title="创建"></a>创建</h2><p>文章统一放在网站根目录下的 <code>_posts</code> 的文件夹中。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-fb74cdc11a950bd4.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>创建一个文件</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-9a47b2074362e570.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>在下面写文章，和标题，还能实时预览，最后提交保存就能看到自己的新文章了。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-88acd9e29fa3ae8a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h2 id="格式"><a href="#格式" class="headerlink" title="格式"></a>格式</h2><p>每一篇文章文件命名采用的是<code>2017-02-04-Hello-2017.md</code>时间+标题的形式，空格用<code>-</code>替换连接。</p><p>文件的格式是 <code>.md</code> 的 <a href="http://sspai.com/25137/" target="_blank" rel="noopener"><strong>MarkDown</strong></a> 文件。</p><p>我们的博客文章格式采用是 <strong>MarkDown</strong>+ <strong>YAML</strong> 的方式。</p><p><a href="http://www.ruanyifeng.com/blog/2016/07/yaml.html?f=tt" target="_blank" rel="noopener"><strong>YAML</strong></a> 就是我们配置 <code>_config</code>文件用的语言。</p><p><a href="http://sspai.com/25137/" target="_blank" rel="noopener"><strong>MarkDown</strong></a> 是一种轻量级的「标记语言」，很简单。<a href="http://sspai.com/25137" target="_blank" rel="noopener">花半个小时看一下</a>就能熟练使用了</p><p>大概就是这么一个结构。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">---</span><br><span class="line">layout:     post       # 使用的布局（不需要改）</span><br><span class="line">title:      My First Post # 标题 </span><br><span class="line">subtitle:   Hello World, Hello Blog #副标题</span><br><span class="line">date:       2017-02-06 # 时间</span><br><span class="line">author:     BY # 作者</span><br><span class="line">header-img: img/post-bg-2015.jpg #这篇文章标题背景图片</span><br><span class="line">catalog: true # 是否归档</span><br><span class="line">tags:#标签</span><br><span class="line">    - 生活</span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">## Hey</span><br><span class="line">&gt;这是我的第一篇博客。</span><br><span class="line"></span><br><span class="line">进入你的博客主页，新的文章将会出现在你的主页上.</span><br></pre></td></tr></table></figure><p>按格式创建文章后，提交保存。进入你的博客主页，新的文章将会出现在你的主页上.</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-f4d5bb65ae3abd00.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>到这里，恭喜你！</p><p>你已经成功搭建了自己的个人博客以及学会在博客上撰写文字的技能了（是不是有点小兴奋🙈）。</p><h4 id="首页标签"><a href="#首页标签" class="headerlink" title="首页标签"></a>首页标签</h4><p>在首页可以看到这些特色标签，当你的文章出现相同标签（默认相同的<strong>标签数量大于1</strong>），才会自动生成。</p><p>所以当你只放一篇文章的时候是不会出现标签的。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-9281b7176c456f92.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>建站的初期，博客比较少，若你想直接在首页生成比较多的标签。你可以在 <code>_congfig.yml</code>中找到这段：</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Featured Tags</span><br><span class="line">featured-tags: true                     # 是否使用首页标签</span><br><span class="line">featured-condition-size: 1              # 相同标签数量大于这个数，才会出现在首页</span><br></pre></td></tr></table></figure><p>将其修改为<code>featured-condition-size: 0</code>, 这样只有一个标签时也会出现在首页了。</p><p>相反，当你博客比较多，标签也很多时，这时你就需要改回 <code>1</code> 甚至是 <code>2</code> 了。</p><h1 id="自定义域名"><a href="#自定义域名" class="headerlink" title="自定义域名"></a>自定义域名</h1><p>搭建好博客之后 你可能不想直接使用 <a href="http://baiyingqiu.github.io" target="_blank" rel="noopener">baiyingqiu.github.io</a> 这么长的博客域名吧, 想换成想 <a href="http://qiubaiying.top" target="_blank" rel="noopener">qiubaiying.top</a> 这样简短的域名。那我们开始吧！</p><h4 id="购买域名"><a href="#购买域名" class="headerlink" title="购买域名"></a>购买域名</h4><p>首先，你必须购买一个自己的域名。</p><p>我是在<a href="https://wanwang.aliyun.com/domain/?spm=5176.8006371.1007.dnetcndomain.q1ys4x" target="_blank" rel="noopener">阿里云</a>购买的域名</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-ef3844cab15e35ff.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>用<strong>阿里云</strong> app也可以注册域名，域名的价格根据后缀的不同和域名的长度而分，比如我这个 <code>qiubaiying.top</code> 的域名第一年才只要4元~</p><p>域名尽量选择短一点比较好记住，注意，不能选择中文域名，比如 <code>张三.top</code> ,GitHub Pages <strong>无法处理中文域名</strong>，会导致你的域名在你的主页上使用。</p><p>注册的步骤就不在介绍了</p><h4 id="解析域名"><a href="#解析域名" class="headerlink" title="解析域名"></a>解析域名</h4><p>注册好域名后，需要将域名解析到你的博客上</p><p>管理控制台 → 域名与网站（万网） → 域名</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-9a75bba50d1b14d7.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>选择你注册好的域名，点击解析</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0968a8dd2045f4fd.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>添加解析</p><p>分别添加两个<code>A</code> 记录类型,</p><p>一个主机记录为 <code>www</code>,代表可以解析 <code>www.qiubaiying.top</code>的域名</p><p>另一个为 <code>@</code>, 代表 <code>qiubaiying.top</code></p><p>记录值就是我们博客的IP地址，是 GitHub Pagas 在美国的服务器的地址 <code>151.101.100.133</code></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0769a93bc487e9d8.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>可以通过 <a href="http://ip.chinaz.com/" target="_blank" rel="noopener">这个网站</a>  或者直接在终端输入<code>ping 你的地址</code>，查看博客的IP</p><pre><code>ping qiubaiying.github.io</code></pre><p>细心地你会发现所有人的博客都解析到 <code>151.101.100.133</code> 这个IP。</p><p>然后 GitHub Pages 再通过 CNAME记录 跳转到你的主页上。</p><h4 id="修改CNAME"><a href="#修改CNAME" class="headerlink" title="修改CNAME"></a>修改CNAME</h4><p>最后一步，只需要修改 我们github仓库下的 <strong>CNAME</strong> 文件。</p><p>选择 <strong>CNAME</strong> 文件</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-a422f3dab436dfb7.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>使用的注册的域名进行替换,然后提交保存</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-6e613004fb410b44.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>这时，输入你自己的域名，就可以解析到你的主页了。</p><p>大功告成！</p><h1 id="进阶"><a href="#进阶" class="headerlink" title="进阶"></a>进阶</h1><p>若你对博客模板进行修改，你就要看看 Jekyll 的<a href="http://jekyll.com.cn" target="_blank" rel="noopener">开发文档</a>,是中文文档哦，对英语一般的朋友简直是福利啊（比如说我😀）。</p><p>还要学习 <strong>Git</strong> 和 <strong>GitHub</strong> 的工作机制了及使用。</p><p>你可以先看看这个<a href="http://www.liaoxuefeng.com/wiki/0013739516305929606dd18361248578c67b8067c8c017b000/" target="_blank" rel="noopener">git教程</a>，对git有个初步的了解后，那么相信你就能将自己图片传到GitHub仓库上，或者可以说掌握了 <strong>使用git管理自己的GitHub仓库</strong> 的技能呢。</p><p>对于轻车熟路的程序猿来说，这篇教程就算就结束了，因为下面的内容对于你们来说 so eazy~</p><p>但相信很多小白都一脸懵逼，那我们继续👇。</p><h1 id="利用GithHub-Desktop管理GitHub仓库"><a href="#利用GithHub-Desktop管理GitHub仓库" class="headerlink" title="利用GithHub Desktop管理GitHub仓库"></a>利用GithHub Desktop管理GitHub仓库</h1><p><a href="https://desktop.github.com/" target="_blank" rel="noopener">GithHub Desktop</a> 是 <strong>GithHub</strong> 推出的一款管理GitHub仓库的桌面软件，换句话说就是将你在<strong>Github</strong>上的文件同步到本地电脑上，并将修改后的文件同步到<strong>Github</strong>远程仓库。</p><h4 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h4><p>点击图片进入下载页面，选择对应的平台进行下载</p><p><a href="https://desktop.github.com/" target="_blank" rel="noopener"><img src="http://upload-images.jianshu.io/upload_images/2178672-6022ba3938b3088e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></a></p><p>下面以<strong>Mac</strong>平台为例：</p><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>将下载好的文件解压，将这只小猫拖到应用程序文件夹中</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-8f8c27f4e5c72276.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>就可以在<strong>Launchpad</strong>找到这只小猫咪~</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0f2da4717361459c.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="登录"><a href="#登录" class="headerlink" title="登录"></a>登录</h4><p>点开应用,会弹出<strong>登录</strong>框，</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-adb7d6824e471ef5.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>输入你的<strong>GitHub</strong>账号和密码进行登录</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-2d7c407ebddbb44f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>登录后关闭窗口</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-93cdccc42024914b.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>然后返回引导窗，一直按 <strong>Continue</strong> 继续</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-450ccef6b1ab7b0a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><strong>Continue</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-06b6e6792472ecae.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>还是<strong>Continue</strong>~<br><img src="http://upload-images.jianshu.io/upload_images/2178672-681a6c455f6b512f.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>进入主界面，先 <strong>右键Remve</strong> 删除这个用户指导，贼烦~</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-604f6f23b8fab6f3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="克隆仓库"><a href="#克隆仓库" class="headerlink" title="克隆仓库"></a>克隆仓库</h4><p>选择你的仓库克隆到本地</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-45ddcd27e2f858a1.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-625be1220fea36b6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="管理仓库"><a href="#管理仓库" class="headerlink" title="管理仓库"></a>管理仓库</h4><p>现在文件夹中打开</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-92c1616af56b501a.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>打开后你会的发现文件结构和你在Github上的一模一样~</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-bf3580ae1cd9a29e.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>你最先关心的可能是你的头像~在<strong>img</strong>文件夹中把替换我的头像就好了。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-c9421d64538c3ba6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>不仅是图片，所有在Github上的的操作都可以进行。</p><h4 id="保存修改"><a href="#保存修改" class="headerlink" title="保存修改"></a>保存修改</h4><p>当你对仓库文件夹的文件下进行修改、添加或删除时，都可以在 <strong>GitHub Desktop</strong> 中看到</p><p>例如我在 <code>img</code> 中添加了一张图片 <code>avatar-demo.png</code> 添加了一张图片</p><p>就可以在看到<strong>GitHub Desktop</strong>显示了我的修改</p><p>保存修改只要按 <strong>Commit to master</strong>，然后可以写上你的修改说明</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-4bfbfec37cbb8eb6.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h4><p>将修改同步到 <strong>GitHub</strong> 远程仓库上只需要一步：点击右上角的<strong>同步按钮</strong></p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-3c2ee8234a7f1832.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="完成"><a href="#完成" class="headerlink" title="完成"></a>完成</h4><p>打开你的GitHub上的仓库，你就可以看到已经和本地同步了</p><p>可以看到你提交的详情： <code>add img</code> </p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-293bdd4cbee0e9e3.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>这样，你已经能轻松管理自己的博客了。</p><p>想上传头像，背景，或者是删掉你不要的图片（我的头像😏）已经是 so eazy了吧~</p><h4 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h4><p>你在 <strong>GitHub</strong> 网站上进行 <strong>Commit</strong> 操作后，需要在<strong>GitHub Desktop</strong>上按一下 <strong>同步按键</strong> 才能同步网站上的修改到你的本地。</p><h1 id="修改个人介绍"><a href="#修改个人介绍" class="headerlink" title="修改个人介绍"></a>修改个人介绍</h1><p><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fme0poz7gqj30vq0l8whh.jpg" alt></p><p>修改个人介绍需要修改根目录下的 <code>about.html</code> 文件</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1fme0rna33tj30bw0bntah.jpg" alt></p><p>看不懂 HTML 标签？没关系，对照着修改就好了~ 还有注意这个有中英介绍</p><p><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fme0sbvmmcj30zp0os7ap.jpg" alt></p><h1 id="常见问题"><a href="#常见问题" class="headerlink" title="常见问题"></a>常见问题</h1><p>最近有很多人给我提问题，我这边总结一下</p><h4 id="配置文件修改后没有效果"><a href="#配置文件修改后没有效果" class="headerlink" title="配置文件修改后没有效果"></a>配置文件修改后没有效果</h4><p>刷新几遍浏览器就好了~</p><p>不行的话，先清除浏览器缓存再试试。</p><h4 id="404错误"><a href="#404错误" class="headerlink" title="404错误"></a>404错误</h4><ol><li>检查你的仓库名是否有按照要求填写</li><li>确定 <strong>Fork</strong> 的是不是我的仓库~</li></ol><h4 id="修改CNAME文件，域名还是不变"><a href="#修改CNAME文件，域名还是不变" class="headerlink" title="修改CNAME文件，域名还是不变"></a>修改CNAME文件，域名还是不变</h4><p>清除浏览器缓存就OK~</p><h4 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h4><p>直接在评论中提出来或私信我，我会一一替大家解决的😀</p><h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><p>最近有人往我的远程仓库不停的 <strong>push</strong>，一天连收几十封邮件！例如像这样的</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-1347f2cc9a4a8dc8.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>原因大多是直接Clone了我的仓库到本地，<strong>没有删除我的远程仓库地址</strong>，添加完自己的仓库地址后，一口气推送到所有远程仓库（包括我的😂）~</p><p>打扰了我的工作和生活~</p><p>所以，<strong>请不要往我的仓库上推送分支</strong>！</p><p>我发现一个问题是，很多人每次修改博客的内容都commit一次到远程仓库，然后再查看修改结果，这样效率非常低！</p><h4 id="来，上车！"><a href="#来，上车！" class="headerlink" title="来，上车！"></a>来，上车！</h4><h2 id="在本地调试博客"><a href="#在本地调试博客" class="headerlink" title="在本地调试博客"></a>在本地调试博客</h2><blockquote><p>注：下面的操作是在 <strong>Mac</strong> 终端进行的。<br><strong>Windows</strong> 环境下的配置请参考 <a href="http://www.jianshu.com/u/a13e7484dc21" target="_blank" rel="noopener">@梦幻之云</a> 提供的 <a href="https://agcaiyun.cn/2017/09/10/%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/" target="_blank" rel="noopener">这篇文章</a>。</p></blockquote><p>有心的同学在 <a href="http://jekyllcn.com/" target="_blank" rel="noopener">jekyll官网</a> 就会发现 <code>jekyll</code> 的 提供的实例代码。</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">~ $ gem install jekyll bundler</span><br><span class="line">~ $ jekyll new my-awesome-site</span><br><span class="line">~ $ cd my-awesome-site</span><br><span class="line">~/my-awesome-site $ bundle install</span><br><span class="line">~/my-awesome-site $ bundle exec jekyll serve</span><br><span class="line"># =&gt; 打开浏览器 http://localhost:4000</span><br></pre></td></tr></table></figure><p>这段命令创建了一个默认的 <code>jekll</code> 网站，然后在本机的 4000 窗口展示。聪明的你应该发现怎么做了吧~</p><p>安装 <code>jekyll</code>和 <code>jekyll bundler</code></p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ gem install jekyll</span><br><span class="line">$ gem install jekyll bundler</span><br></pre></td></tr></table></figure><p>进入你的 <strong>Blog 所在目录</strong>，然后创建本地服务器</p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ jekyll s</span><br></pre></td></tr></table></figure><p>然后会显示 </p><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"> Auto-regeneration: enabled for &apos;/Users/baiying/Blog&apos;</span><br><span class="line">Configuration file: /Users/baiying/Blog/_config.yml</span><br><span class="line">    Server address: http://127.0.0.1:4000/</span><br><span class="line">  Server running... press ctrl-c to stop.</span><br></pre></td></tr></table></figure><p>你就可以在 <a href="http://127.0.0.1:4000/" target="_blank" rel="noopener">http://127.0.0.1:4000/</a> 看到你的博客，你对本地博客的修改都会在这个地址进行显示，这大大提高了对博客的配置效率。</p><p>使用<code>ctrl+c</code>就可以停止 <strong>serve</strong></p><h1 id="Star"><a href="#Star" class="headerlink" title="Star"></a>Star</h1><p>若本教程顺利帮你搭建了自己的个人博客，请不要 <strong>害羞</strong>，给我的 <a href="https://github.com/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener">github仓库</a> 点个 <strong>star</strong> 吧！</p><p>因为最近发现 Fork 将近破百，加上直接 Clone 仓库的，保守估计已经帮助上百人成功的搭建了自己的博客，<del>可是 Star 却仅仅只有 <strong>12</strong>！可能还是做的不够好吧！</del>现在已经破百了，感谢大家的Star！</p><h3 id="别无他求，点个-Star-吧！"><a href="#别无他求，点个-Star-吧！" class="headerlink" title="别无他求，点个 Star 吧！"></a><strong>别无他求，点个 <a href="https://github.com/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener">Star</a> 吧</strong>！</h3><p><img src="http://upload-images.jianshu.io/upload_images/2178672-768a38ee9fb0df28.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p><strong>心满意足！</strong></p><h1 id="补充"><a href="#补充" class="headerlink" title="补充"></a>补充</h1><h4 id="修改网站的-icon"><a href="#修改网站的-icon" class="headerlink" title="修改网站的 icon"></a>修改网站的 <strong>icon</strong></h4><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1flgh6k23ppj30ad00uq2t.jpg" alt></p><p>要修改如图所示的网站 <strong>icon</strong>：</p><p>在博客 <code>img</code> 目录下找到并替换 <code>favicon.ico</code> 这个图标即可，图标尺寸为<code>32x32</code>。</p><p><img src="https://ws2.sinaimg.cn/large/006tNc79gy1flghahch1oj30gu09y419.jpg" alt></p><h4 id="修改主页的座右铭"><a href="#修改主页的座右铭" class="headerlink" title="修改主页的座右铭"></a>修改主页的座右铭</h4><p>最近有不少小伙伴私信我：<strong>如何修改主页的座右铭？</strong></p><p>就是这个：</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-31dc0068f256aca3.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>很简单，找到博客目录下的 <strong>index.html</strong> 文件，修改这句话就可以了。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-9e4785654523bf07.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h4 id="如何在博客文章中上插入图片"><a href="#如何在博客文章中上插入图片" class="headerlink" title="如何在博客文章中上插入图片"></a>如何在博客文章中上插入图片</h4><p>博客的文章用的是 MarkDown 格式，如果没用过 MarkDown 真的 强烈推荐 <a href="http://sspai.com/25137" target="_blank" rel="noopener">花半个小时学习一下</a>。</p><p>MarkDown 中添加图片的形式是 :<code>![](图片的URL)</code></p><p>例如：</p><p><code>![MarkDown示例图片](http://upload-images.jianshu.io/upload_images/2178672-eb2effd6b942a500.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)</code>就会显示下面这张图片</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-98965f66db8f5856.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MarkDown示例图片"></p><p><code>https://ws3.sinaimg.cn/large/006tNc79gy1fj9xhjzobbj30yg0my75z.jpg</code>就是这张图片的URL，我们可以在浏览器输入这个URL找到或下载这张图片。</p><p>所以，要在 MacDown 中插入图片，这张图片就需要上传到图床（网上），然后在引<br>用这张图片的URL。</p><h5 id="将图片上传到图床"><a href="#将图片上传到图床" class="headerlink" title="将图片上传到图床"></a>将图片上传到图床</h5><p>Mac 上的图床神器：iPic  </p><p>直接在App Store上下载，谁用谁知道！</p><p>使用方法很简单，直接拖动图片到 P 图标上，或者选中图片按快捷键 <code>⌘+U</code>，就能请示上传。</p><p>上传成功就能直接粘贴图片的URL。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-7399aeaced6f1e29.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="iPic"></p><p>用 iPic 上传图片后，获取URL插入文章中就可以了。</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-4be76fb02708de5e.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="iPic上传图片"></p><h4 id="推荐几个好用软件"><a href="#推荐几个好用软件" class="headerlink" title="推荐几个好用软件"></a>推荐几个好用软件</h4><h5 id="MarkDown编辑器"><a href="#MarkDown编辑器" class="headerlink" title="MarkDown编辑器"></a>MarkDown编辑器</h5><p><a href="https://macdown.uranusjr.com/" target="_blank" rel="noopener">MacDown</a>：可能是Mac上最好的MacDown编辑器了  </p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-2226239a63278302.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><h5 id="图片压缩工具"><a href="#图片压缩工具" class="headerlink" title="图片压缩工具"></a>图片压缩工具</h5><p><a href="https://imageoptim.com/" target="_blank" rel="noopener">ImageOptim</a></p><p>对于我们的博客来说，图片越大，加载速度越慢。</p><p>不信你用手机打开你的博客试试~</p><p>所以有必要对我们上传到博客网站中的图片：指的是你的头像，首页背景图片，文章背景图片等。对于博客文章中插入的图片，其实也可以压缩了再上传。</p><p>对博客中的所有图片进行压缩：</p><p>看看压缩结果，最高的一张压缩了78.7%，这简直是太可怕了！</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-0f8e643fa1da8674.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="ImageOptim压缩图片"></p><p>好了，现在个人博客的加载速度估计要起飞了~</p><h1 id="最后要说个事情"><a href="#最后要说个事情" class="headerlink" title="最后要说个事情"></a>最后要说个事情</h1><p>我在博客中的文章，你们可以保留，让更多需要帮助人的看到，当然也可以删除。</p><p>但是，我发现居然有人把文章的作者改成了自己，然后当成自己的文章放在自己的博客上，这就令人感到气愤了。</p><p>比如说向我请教问题的这位：</p><p><img src="http://upload-images.jianshu.io/upload_images/2178672-ed45ebafec7f5d34.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt></p><p>我在博客中的每篇文章都是我一字一句敲出来的，转载的文章我也注明了出处，表示对原作者的尊重。同时也希望大家都能尊重我的付出。</p><p>谢谢~</p><p><img src="https://raw.githubusercontent.com/qiubaiying/qiubaiying.github.io/master/img/readme-home.png" alt></p><p><a href="https://travis-ci.org/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener"><img src="https://travis-ci.org/qiubaiying/qiubaiying.github.io.svg?branch=master" alt="Build Status"></a><br><a href="https://codebeat.co/projects/github-com-qiubaiying-qiubaiying-github-io-master" target="_blank" rel="noopener"><img src="https://codebeat.co/badges/5f031df3-f6c1-4ec0-911a-ff6617ca50b9" alt="codebeat badge"></a><br><a href="https://github.com/qiubaiying/qiubaiying.github.io/issues" target="_blank" rel="noopener"><img src="https://img.shields.io/github/issues/qiubaiying/qiubaiying.github.io.svg?style=flat" alt="GitHub issues"></a><br><a href="https://github.com/home-assistant/home-assistant-iOS/blob/master/LICENSE" target="_blank" rel="noopener"><img src="https://img.shields.io/badge/license-MIT-blue.svg?style=flat" alt="License MIT"></a><br><a href="https://github.com/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener"><img src="https://img.shields.io/github/stars/qiubaiying/qiubaiying.github.io.svg?style=social&amp;label=Star" alt></a><br><a href="https://github.com/qiubaiying/qiubaiying.github.io" target="_blank" rel="noopener"><img src="https://img.shields.io/github/forks/qiubaiying/qiubaiying.github.io.svg?style=social&amp;label=Fork" alt></a></p><p>博客的搭建教程修改自 <a href="https://github.com/Huxpro/huxpro.github.io" target="_blank" rel="noopener">Hux</a> </p><p>更为详细的教程戳这 <a href="http://www.jianshu.com/p/e68fba58f75c" target="_blank" rel="noopener">《利用 GitHub Pages 快速搭建个人博客》</a> 或 <a href="https://github.com/qiubaiying/qiubaiying.github.io/wiki/%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%95%99%E7%A8%8B" target="_blank" rel="noopener">wiki</a></p><blockquote></blockquote><h3 id="查看博客戳这里-👆"><a href="#查看博客戳这里-👆" class="headerlink" title="查看博客戳这里 👆"></a><a href="http://qiubaiying.github.io" target="_blank" rel="noopener">查看博客戳这里 👆</a></h3>]]></content>
      
      
      
        <tags>
            
            <tag> Blog </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
