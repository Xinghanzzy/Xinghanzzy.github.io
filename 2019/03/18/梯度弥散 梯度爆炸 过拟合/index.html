<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head><meta name="generator" content="Hexo 3.8.0">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="何言丝竹">
    <meta name="keyword" content="NLP,机器翻译,深度学习">
    <meta name="theme-color" content="#600090">
    <meta name="msapplication-navbutton-color" content="#600090">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="#600090">
    <link rel="shortcut icon" href="/images/favicon3.png">
    <link rel="alternate" type="application/atom+xml" title="Boxiao" href="/atom.xml">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/animate.css/3.5.2/animate.min.css">
    <link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.css">
    <title>
        
        深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize｜Boxiao&#39;s blog
        
    </title>

    <link rel="canonical" href="http://xinghanzzy.github.io/2019/03/18/梯度弥散 梯度爆炸 过拟合/">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/blog-style.css">

    <!-- Pygments Github CSS -->
    <link rel="stylesheet" href="/css/syntax.css">
</head>

<style>

    header.intro-header {
        background-image: url('/images/lion-blur-bg.jpg')
    }
</style>
<!-- hack iOS CSS :active style -->
<body ontouchstart="" class="animated fadeIn">
<!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top " id="nav-top" data-ispost="true" data-istags="false
" data-ishome="false">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand animated pulse" href="/">
                <span class="brand-logo">
                    Boxiao
                </span>
                's Blog
            </a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <!-- Known Issue, found by Hux:
            <nav>'s height woule be hold on by its content.
            so, when navbar scale out, the <nav> will cover tags.
            also mask any touch event of tags, unfortunately.
        -->
        <!-- /.navbar-collapse -->
        <div id="huxblog_navbar">
            <div class="navbar-collapse">
                <ul class="nav navbar-nav navbar-right">
                    <li>
                        <a href="/">Home</a>
                    </li>
					
                    
                        
							
                        <li>
                            <a href="/tags/">tags</a>
                        </li>
							
						
                    
					
					
                </ul>
            </div>
        </div>
    </div>
    <!-- /.container -->
</nav>
<script>
    // Drop Bootstarp low-performance Navbar
    // Use customize navbar with high-quality material design animation
    // in high-perf jank-free CSS3 implementation
//    var $body   = document.body;
    var $toggle = document.querySelector('.navbar-toggle');
    var $navbar = document.querySelector('#huxblog_navbar');
    var $collapse = document.querySelector('.navbar-collapse');

    $toggle.addEventListener('click', handleMagic)
    function handleMagic(e){
        if ($navbar.className.indexOf('in') > 0) {
        // CLOSE
            $navbar.className = " ";
            // wait until animation end.
            setTimeout(function(){
                // prevent frequently toggle
                if($navbar.className.indexOf('in') < 0) {
                    $collapse.style.height = "0px"
                }
            },400)
        }else{
        // OPEN
            $collapse.style.height = "auto"
            $navbar.className += " in";
        }
    }
</script>

<!-- Main Content -->

<!--only post-->


<img class="wechat-title-img" src="null/images/post-bg-kuaidi.jpg">


<style>
    
    header.intro-header {
        background-image: url('/images/post-bg-kuaidi.jpg?imageView2/1/w/1400/h/400/interlace/1/q/90')
    }

    
</style>

<header class="intro-header">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <div class="post-heading">
                    <h1>深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize</h1>
                    
                    <h2 class="subheading">梯度弥散 梯度爆炸 过拟合 batchsize Adam Attention</h2>
                    
                    <span class="meta">
                         作者 XH
                        <span>
                          日期 2019-03-18
                         </span>
                    </span>
                    <div class="tags text-center">
                        
                        <a class="tag" href="/tags/#NMT" title="NMT">NMT</a>
                        
                        <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                        
                    </div>
                </div>
            </div>
        </div>
    </div>
    <div class="post-title-haojen">
        <span>
            深度学习概念：梯度弥散 梯度爆炸 过拟合 batchsize
        </span>
    </div>
</header>

<!-- Post Content -->
<article>
    <div class="container">
        <div class="row">
            <!-- Post Container -->
            <div class="col-lg-8 col-lg-offset-1 col-sm-9 post-container">
                <blockquote>
<p>自己看的记录笔记</p>
<p>参考文献</p>
<p><a href="https://blog.csdn.net/zhangbaoanhadoop/article/details/82290129" target="_blank" rel="noopener">解决梯度消失和梯度弥散的方法</a> </p>
<p><a href="https://www.cnblogs.com/robert-dlut/p/5952032.html/" target="_blank" rel="noopener">注意力机制（Attention Mechanism）在自然语言处理中的应用</a></p>
<p><a href="https://zhuanlan.zhihu.com/p/40920384" target="_blank" rel="noopener">真正的完全图解Seq2Seq Attention模型</a></p>
<p><a href="https://blog.csdn.net/u010089444/article/details/76725843" target="_blank" rel="noopener">优化方法总结：SGD，Momentum，AdaGrad，RMSProp，Adam</a></p>
<p><a href="https://blog.csdn.net/shuzfan/article/details/75675568" target="_blank" rel="noopener">梯度下降优化算法总结</a></p>
</blockquote>
<h1 id="深度学习概念"><a href="#深度学习概念" class="headerlink" title="深度学习概念"></a>深度学习概念</h1><h2 id="梯度弥散-梯度爆炸"><a href="#梯度弥散-梯度爆炸" class="headerlink" title="梯度弥散 梯度爆炸"></a>梯度弥散 梯度爆炸</h2><p>​    梯度弥散。使用反向传播算法传播梯度的时候，随着传播深度的增加，梯度的幅度会急剧减小，会导致浅层神经元的权重更新非常缓慢，不能有效学习。这样一来，深层模型也就变成了前几层相对固定，只能改变最后几层的浅层模型。  </p>
<p>​    梯度弥散的问题很大程度上是来源于激活函数的“饱和”。 </p>
<p><strong>梯度消失：</strong></p>
<blockquote>
<ol>
<li>Relu代替sigmoid(梯度消失 )</li>
<li>梯度裁剪、正则(损失函数、针对梯度爆炸 )</li>
<li>RNN-LSTM</li>
<li>一种新的方法batch normalization</li>
<li>ResNet残差结构</li>
<li>预训练+微调</li>
</ol>
</blockquote>
<p><strong>梯度爆炸：</strong></p>
<blockquote>
<p> 使用Gradient Clipping(梯度裁剪)。通过Gradient Clipping，将梯度约束在一个范围内，这样不会使得梯度过大。 </p>
<p>原来的网络，如果简单地<strong>增加深度</strong>，会导致梯度弥散或梯度爆炸。对于该问题的解决方法是正则化初始化和中间的正则化层（Batch <strong>Normalization</strong>），这样的话可以训练几十层的网络。</p>
<p>虽然通过上述方法能够训练了，但是又会出现另一个问题，就是退化问题，网络层数增加，但是在训练集上的准确率却饱和甚至下降了。这个不能解释为overfitting，因为overfit应该表现为在训练集上表现更好才对。退化问题说明了深度网络不能很简单地被很好地优化。</p>
<p><strong>residual</strong></p>
</blockquote>
<h2 id="过拟合"><a href="#过拟合" class="headerlink" title="过拟合"></a>过拟合</h2><p>在训练数据不够多时，或者overtraining时，常常会导致overfitting（过拟合）。其直观的表现如下图所示，随着训练过程的进行，模型复杂度增加，在training data上的error渐渐减小，但是在验证集上的error却反而渐渐增大——因为训练出来的网络过拟合了训练集，对训练集外的数据却不work。</p>
<p>为了防止overfitting，可以用的方法有很多，下文就将以此展开。有一个概念需要先说明，在机器学习算法中，我们常常将原始数据集分为三部分：training data、validation data，testing data。这个validation data是什么？它其实就是用来避免过拟合的，在训练过程中，我们通常用它来确定一些超参数（比如根据validation data上的accuracy来确定early stopping的epoch大小、根据validation data确定learning rate等等）。那为啥不直接在testing data上做这些呢？因为如果在testing data做这些，那么随着训练的进行，我们的网络实际上就是在一点一点地overfitting我们的testing data，导致最后得到的testing accuracy没有任何参考意义。因此，training data的作用是计算梯度更新权重，validation data如上所述，testing data则给出一个accuracy以判断网络的好坏。</p>
<p>避免过拟合的方法有很多：early stopping、数据集扩增（Data augmentation）、正则化（Regularization）包括L1、L2（L2 regularization也叫weight decay），dropout。</p>
<h3 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h3><ul>
<li>所谓过拟合（Overfit），是这样一种现象：一个假设在训练数据上能够获得比其他假设更好的拟合，但是在训练数据外的数据集 上却不能很好的拟合数据。此时我们就叫这个假设出现了overfit的现象。 </li>
<li>当一个模型过为复杂之后，它可以很好的记忆每一个训练数据中的随机噪声，却忘记要去学习训练数据中的通用趋势</li>
</ul>
<h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ul>
<li>过拟合其中一个可能的成因就是模型的vc维过高，使用了过强的模型复杂度(model complexity)的能力。（参数多并且过训练）  　　</li>
<li>还有一个原因是数据中的噪声，造成了如果完全拟合的话，也许与真实情景的偏差更大。  　　</li>
<li>最后还有一个原因是数据量有限，这使得模型无法真正了解整个数据的真实分布。  　　<ul>
<li>权值学习迭代次数足够多(Overtraining),拟合了训练数据中的噪声和训练样例中没有代表性的特征。     </li>
</ul>
</li>
</ul>
<h3 id="解决"><a href="#解决" class="headerlink" title="解决"></a>解决</h3><ol>
<li><p>权值衰减  　　在每次迭代过程中以某个小因子降低每个权值,这等效于修改E的定义,加入一个与网络权值的总量相应的惩罚项,此方法的动机是保持权值较小,避免weight decay,从而使学习过程向着复杂决策面的反方向偏。<strong>（L2正则化）</strong>  </p>
</li>
<li><p>适当的stopping criterion（验证集） </p>
</li>
<li><p><strong>交叉验证方法</strong></p>
<p>在可获得额外的数据提供验证集合时工作得很好,但是小训练集合的过度拟合问题更为严重 </p>
<p><strong>k-fold交叉方法</strong>:  　　把训练样例分成k份,然后进行k次交叉验证过程,每次使用不同的一份作为验证集合,其余k-1份合并作为训练集合.每个样例会在一次实验中被用作验证样例,在k-1次实验中被用作训练样例;每次实验中,使用上面讨论的交叉验证过程来决定在验证集合上取得最佳性能的迭代次数n*,然后计算这些迭代次数的均值,作为最终需要的迭代次数。 </p>
</li>
<li><p><strong>正则化</strong> </p>
<p>​    正则化是一种回归的形式，它将系数估计（coefficient estimate）朝零的方向进行约束、调整或缩小。也就是说，正则化可以在学习过程中降低模型复杂度和不稳定程度，从而避免过拟合的危险。 </p>
<p>​    L1和L2正则都是比较常见和常用的正则化项，都可以达到<strong>防止过拟合</strong>的效果。L1正则化的解具有<strong>稀疏性</strong>，可用于<strong>特征选择</strong>。L2正则化的解都比较小，<strong>抗扰动能力强</strong>。 </p>
<p>​    <strong>使用L2正则项的解不具有稀疏性</strong> ，L2缺点：模型的可解释性。它将把不重要的预测因子的系数缩小到趋近于 0，但永不达到 0。也就是说，最终的模型会包含所有的预测因子</p>
</li>
</ol>
<p>   ​    正则化方法是在损失函数时候改变</p>
<script type="math/tex; mode=display">
   J(\theta) = J(\theta) + \lambda R(w)</script><script type="math/tex; mode=display">
   R(w) = ||w||_1=\sum_{i}{|w_i|}</script><script type="math/tex; mode=display">
   R(w) = ||w||_2^2=\sum_{i}{|w_i^2|}   \text {L2正则}</script><p>   ​    通过限制权重的大小，使得模型不能任意拟合训练数据中的随机噪声。</p>
<p>   ​    <strong>L2 regularizer</strong> ：使得模型的解偏向于范数较小的 W，通过限制 W 范数的大小实现了对模型空间的限制，从而在一定程度上避免了 overfitting 。不过 ridge regression 并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。</p>
<p>   ​    <strong>L1 regularizer</strong> ：它的优良性质是能产生稀疏性，导致 W 中许多项变成零。 稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。</p>
<p>   L1 的优点: 能够获得更加稀疏的模型.<br>   L1 的缺点: 加入 L1 后会使得目标函数在原点不可导, 需要做特殊处理</p>
<p>   L2 的有点: 在任意位置都可导, 优化求解过程比较方便, 而且更加稳定<br>   L2 的缺点: 无法获得真正的稀疏模型</p>
<p>   <strong>在实际应用过程中, 大部分情况下都是 L2 正则的效果更好, 因此推荐优先使用 L2 正则</strong></p>
<p>   <strong>正则化是通过约束参数的范数使其不要太大，所以可以在一定程度上减少过拟合情况。</strong> </p>
<ol>
<li><p><strong>Dropout正则化</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">部分置为：0</span><br><span class="line">其他缩放：1 / (1 - rate)</span><br></pre></td></tr></table></figure>
</li>
<li><p>数据！</p>
<blockquote>
<p>（1）在神经网络模型中，可使用权值衰减的方法，即每次迭代过程中以某个小因子降低每个权值。</p>
<p>（2）选取合适的停止训练标准，使对机器的训练在合适的程度；</p>
<p>（3）保留验证数据集，对训练成果进行验证；</p>
<p>（4）获取额外数据进行交叉验证；</p>
<p>（5）正则化，即在进行目标函数或代价函数优化时，在目标函数或代价函数后面加上一个正则项，一般有L1正则与L2正则等</p>
</blockquote>
</li>
</ol>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><ul>
<li>基于模型的方法: 采用降低过拟合风险的措施,包括简化模型(如将非线性简化成线性), 添加约束项以缩小假设空间(如L1和L2正则化), 集成学习, Dropout超参数等.</li>
<li>基于数据的方法, 主要通过数据扩充(Data Augmentation), 即根据一些先验知识, 在保持特定信息的前提下, 对原始数据进行适合变换以达到扩充数据集的效果.</li>
</ul>
<h2 id="Batch-size-amp-SGD"><a href="#Batch-size-amp-SGD" class="headerlink" title="Batch size&amp;SGD"></a>Batch size&amp;SGD</h2><h3 id="Batch-Size三种情况（SGD）"><a href="#Batch-Size三种情况（SGD）" class="headerlink" title="Batch_Size三种情况（SGD）"></a>Batch_Size三种情况（SGD）</h3><p>　　Batch_Size（批尺寸）是机器学习中一个重要参数。 </p>
<ol>
<li><p><strong>Batch Gradient Descent</strong></p>
<p>如果数据集比较小，完全可以采用全数据集 （ Full Batch Learning ）的形式，这样做至少有 2 个好处：其一，由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。其二，由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。 </p>
<blockquote>
<ul>
<li><p>凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点 </p>
</li>
<li><p>每次学习时间过长 </p>
</li>
<li>训练集很大以至于需要消耗大量的内存 </li>
<li>全量梯度下降不能进行在线模型参数更新 </li>
</ul>
</blockquote>
</li>
<li><p><strong>Stochastic Gradient Descent</strong></p>
<p>Batch_Size = 1。这就是在线学习（Online Learning）。使用在线学习，每次修正方向以各自样本的梯度方向修正，横冲直撞<a href="https://www.baidu.com/s?wd=%E5%90%84%E8%87%AA%E4%B8%BA%E6%94%BF&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">各自为政</a>，难以达到收敛。 </p>
<blockquote>
<ul>
<li>算法收敛速度快 </li>
<li><p>可以在线更新</p>
</li>
<li><p>最大的缺点在于每次更新可能并不会按照正确的方向进行，因此可以带来优化波动(扰动) </p>
</li>
<li>容易收敛到局部最优，并且容易被困在鞍点</li>
</ul>
</blockquote>
</li>
<li><p><strong>Mini-batch Gradient Descent</strong></p>
<p>如果网络中采用minibatch SGD算法来优化，所以是一个batch一个batch地将数据输入CNN模型中，然后计算这个batch的所有样本的平均损失，即代价函数是所有样本的平均。而batch_size就是一个batch的所包含的样本数，显然batch_size将影响到模型的优化程度和速度。<strong>mini batch只是为了充分利用GPU memory而做出的妥协</strong> </p>
<blockquote>
<ul>
<li><p>选择一个合理的学习速率很难。如果学习速率过小，则会导致收敛速度很慢。如果学习速率过大，那么其会阻碍收敛，即在极值点附近会振荡。</p>
</li>
<li><p>学习速率调整(又称学习速率调度，Learning rate schedules)[11]试图在每次更新过程中，改变学习速率，如退火。一般使用某种事先设定的策略或者在每次迭代中衰减一个较小的阈值。无论哪种调整方法，都需要事先进行固定设置，这边便无法自适应每次学习的数据集特点[10]。</p>
</li>
<li><p>模型所有的参数每次更新都是使用相同的学习速率。如果数据特征是稀疏的或者每个特征有着不同的取值统计特征与空间，那么便不能在每次更新中每个参数使用相同的学习速率，那些很少出现的特征应该使用一个相对较大的学习速率。</p>
</li>
<li><p>对于非凸目标函数，容易陷入那些次优的局部极值点中，如在神经网路中。那么如何避免呢。</p>
</li>
</ul>
</blockquote>
</li>
</ol>
<h3 id="改变Batch-Size的影响"><a href="#改变Batch-Size的影响" class="headerlink" title="改变Batch_Size的影响"></a>改变Batch_Size的影响</h3><p>　　在合理范围内，增大 Batch_Size 的好处：内存利用率提高了，大矩阵乘法的并行化效率提高。跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。<strong>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小</strong>。 
　　</p>
<blockquote>
<p>下面是参考文献中博主给出的实验结果： </p>
<p>Batch_Size 太小，算法在 200 epoches 内不收敛。 </p>
<p>随着 Batch_Size 增大，处理相同数据量的速度越快。 </p>
<p>随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。</p>
<p>由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。 </p>
<p>由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。</p>
</blockquote>
<p>　　当采用mini-batch时，我们可以将一个batch里的所有样本放在一个矩阵里，利用线性代数库来加速梯度的计算，这是工程实现中的一个优化方法。<br>　　一个大的batch，可以充分利用矩阵、线性代数库来进行计算的加速，batch越小，则加速效果可能越不明显。当然batch也不是越大越好，太大了，权重的更新就会不那么频繁，导致优化过程太漫长。</p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>SGD方法的一个缺点是，其<strong>更新方向</strong>完全依赖于当前的batch，因而其更新十分不稳定。</p>
<p>momentum，动量，它<a href="https://www.baidu.com/s?wd=%E6%A8%A1%E6%8B%9F&amp;tn=24004469_oem_dg&amp;rsv_dl=gh_pl_sl_csd" target="_blank" rel="noopener">模拟</a>的是物体运动时的惯性，即更新的时候在一定程度上保留之前更新的方向，同时利用当前batch的梯度微调最终的更新方向。这样一来，可以在一定程度上增加稳定性，从而学习地更快，并且还有一定摆脱局部最优的能力。</p>
<p>利用惯性，即当前梯度与上次梯度进行加权，如果方向一致，则累加导致更新步长变大；如果方向不同，则相互抵消中和导致更新趋向平衡。</p>
<p>动量参数常被设定为0.9或者一个相近的值。</p>
<blockquote>
<p>Nesterov Momentum</p>
<p>在小球向下滚动的过程中，我们希望小球能够提前知道在哪些地方坡面会上升，这样在遇到上升坡面之前，小球就开始减速。这方法就是Nesterov Momentum，其在<strong>凸优化</strong>中有较强的理论保证收敛。 </p>
</blockquote>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad算法能够在训练中自动的对learning rate进行调整，对于出现频率较低参数采用较大的α更新</p>
<p>适合处理<strong>稀疏数据</strong> </p>
<p>对角矩阵 每个对角线位置是1-&gt;t轮梯度平方和</p>
<p>缺点是在训练的中后期，分母上梯度平方的累加将会越来越大，从而梯度趋近于0，使得训练提前结束 </p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adadelta： <strong>仅采用一个窗口范围内的梯度平方之和</strong> </p>
<p><strong>Adadelta已经无需设置初始学习率了，其可以自动计算并更新学习率</strong> </p>
<h2 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h2><p>Adagrad会累加之前所有的梯度平方，而RMSprop仅仅是计算对应的平均值，因此可缓解Adagrad算法学习率下降较快的问题 </p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><blockquote>
<p>梯度下降（Gradient Descent）就好比一个人想从高山上奔跑到山谷最低点，用最快的方式（steepest）奔向最低的位置（minimum） </p>
<p>​     1.Adam算法可以看做是修正后的<a href="http://blog.csdn.net/bvl10101111/article/details/72615621" target="_blank" rel="noopener">Momentum</a>+<a href="http://blog.csdn.net/BVL10101111/article/details/72616378" target="_blank" rel="noopener">RMSProp</a>算法</p>
<p>​     2.动量直接并入梯度一阶矩估计中（指数加权）</p>
<p>​     3.Adam通常被认为对超参数的选择相当鲁棒</p>
<p>​     4.学习率建议为0.001</p>
</blockquote>
<p>Adam 算法和传统的随机梯度下降不同。随机梯度下降保持单一的学习率（即 alpha）更新所有的权重，学习率在训练过程中并不会改变。而 Adam 通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率 </p>
<p>Adam(Adaptive Moment Estimation)是另一种自适应学习率的方法。它利用梯度的一阶矩估计和二阶矩估计动态调整每个参数的学习率。Adam的优点主要在于经过偏置校正后，每一次迭代学习率都有个确定范围，使得参数比较平稳。 </p>
<p><strong>Adam = Momentum+RMSProp</strong>  一阶 + 二阶</p>
<p>度的一阶矩估计（First Moment Estimation，即梯度的均值）和二阶矩估计（Second Moment Estimation，即梯度的未中心化的方差）进行综合考虑</p>
<blockquote>
<ol>
<li><p>实现简单，计算高效，对内存需求少</p>
</li>
<li><p>参数的更新不受梯度的伸缩变换影响</p>
</li>
<li><p>超参数具有很好的解释性，且通常无需调整或仅需很少的微调</p>
</li>
<li><p>更新的步长能够被限制在大致的范围内（初始学习率）</p>
</li>
<li><p>能自然地实现步长退火过程（自动调整学习率）</p>
</li>
<li><p>很适合应用于大规模的数据及参数的场景</p>
</li>
<li><p>适用于不稳定目标函数</p>
</li>
<li><p>适用于梯度稀疏或梯度存在很大噪声的问题</p>
</li>
</ol>
<p>综合Adam在很多情况下算作默认工作性能比较优秀的优化器。</p>
<p>warmup 更新学习率</p>
<p>Adam 快！</p>
</blockquote>
<h2 id="attention-乘法-加法"><a href="#attention-乘法-加法" class="headerlink" title="attention 乘法 加法"></a>attention 乘法 加法</h2><blockquote>
<p>所谓乘法 加法 做的是decoder的隐藏状态S_{t-1}和encoder的各个隐层状态h_j</p>
<p>这里</p>
</blockquote>
<script type="math/tex; mode=display">
P(y_i|y_1,...y_{i-1},X) = g(y_{i-1},s_i,c_i)</script><p><img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g1zvqx6ma6j20di07u0to.jpg" alt> <img src="https://ws1.sinaimg.cn/large/4ac7f217ly1g1zvsjhqjej20du07ewff.jpg" alt></p>
<h2 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h2><p>使用sigmod控制</p>
<h2 id="数据不平衡"><a href="#数据不平衡" class="headerlink" title="数据不平衡"></a>数据不平衡</h2><p><strong>大数据+分布均衡&lt;大数据+分布不均衡&lt;小数据+数据均衡&lt;小数据+数据不均衡</strong> </p>
<p>解决这一问题的基本思路是让正负样本在训练过程中拥有相同的话语权，比如利用采样与加权等方法。为了方便起见，我们把数据集中样本较多的那一类称为“大众类”，样本较少的那一类称为“小众类” </p>
<ol>
<li><p><strong>采样</strong></p>
<p>采样方法是通过对训练集进行处理使其从不平衡的数据集变成平衡的数据集，在大部分情况下会对最终的结果带来提升。 </p>
<p><strong>上采样（Oversampling）</strong>和<strong>下采样（Undersampling）</strong>，上采样是把小种类复制多份，下采样是从大众类中剔除一些样本，或者说只从大众类中选取部分样本。 </p>
<p>上采样后的数据集中会反复出现一些样本，训练出来的模型会有一定的过拟合；</p>
<ol>
<li>可以在每次生成新数据点时加入轻微的随机扰动 </li>
</ol>
<p>下采样的缺点显而易见，那就是最终的训练集丢失了数据，模型只学到了总体模式的一部分。 </p>
<ol>
<li>第一种方法叫做EasyEnsemble，利用模型融合的方法（Ensemble）：多次下采样（放回采样，这样产生的训练集才相互独立）产生多个不同的训练集，进而训练多个不同的分类器，通过组合多个分类器的结果得到最终的结果。</li>
<li>第二种方法叫做BalanceCascade，利用增量训练的思想（Boosting）：先通过一次下采样产生训练集，训练一个分类器，对于那些分类正确的大众样本不放回，然后对这个更小的大众样本下采样产生训练集，训练第二个分类器，以此类推，最终组合所有分类器的结果得到最终结果。</li>
<li>第三种方法是利用KNN试图挑选那些最具代表性的大众样本，叫做NearMiss，这类方法计算量很大 </li>
</ol>
</li>
<li><p><strong>数据合成</strong></p>
</li>
<li><p><strong>加权</strong></p>
</li>
</ol>
<ol>
<li><strong>一分类</strong></li>
</ol>

                <hr>
                

                <ul class="pager">
                    
                    <li class="previous">
                        <a href="/2019/04/08/相对位置 relative position/" data-toggle="tooltip" data-placement="top" title="self-attention 相对位置 relative position.">&larr; Previous Post</a>
                    </li>
                    
                    
                    <li class="next">
                        <a href="/2019/03/14/Transformer norm 先做后做/" data-toggle="tooltip" data-placement="top" title="Transformer norm 先做后做">Next Post &rarr;</a>
                    </li>
                    
                </ul>

                

                


                <!--加入新的评论系统-->
                

                

            </div>

            <div class="hidden-xs col-sm-3 toc-col">
                <div class="toc-wrap">
                    <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#深度学习概念"><span class="toc-text">深度学习概念</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#梯度弥散-梯度爆炸"><span class="toc-text">梯度弥散 梯度爆炸</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#过拟合"><span class="toc-text">过拟合</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#定义"><span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#原因"><span class="toc-text">原因</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#解决"><span class="toc-text">解决</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#解决方法"><span class="toc-text">解决方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Batch-size-amp-SGD"><span class="toc-text">Batch size&amp;SGD</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Batch-Size三种情况（SGD）"><span class="toc-text">Batch_Size三种情况（SGD）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#改变Batch-Size的影响"><span class="toc-text">改变Batch_Size的影响</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Momentum"><span class="toc-text">Momentum</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adagrad"><span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adadelta"><span class="toc-text">Adadelta</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#RMSprop"><span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Adam"><span class="toc-text">Adam</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#attention-乘法-加法"><span class="toc-text">attention 乘法 加法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GRU"><span class="toc-text">GRU</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#数据不平衡"><span class="toc-text">数据不平衡</span></a></li></ol></li></ol>
                </div>
            </div>
        </div>

        <div class="row">
            <!-- Sidebar Container -->

            <div class="
                col-lg-8 col-lg-offset-2
                col-md-10 col-md-offset-1
                sidebar-container">

                <!-- Featured Tags -->
                
                <section>
                    <!-- no hr -->
                    <h5 class="text-center">
                        <a href="/tags/">FEATURED TAGS</a>
                    </h5>
                    <div class="tags">
                        
                        <a class="tag" href="/tags/#NMT" title="NMT">NMT</a>
                        
                        <a class="tag" href="/tags/#深度学习" title="深度学习">深度学习</a>
                        
                    </div>
                </section>
                

                <!-- Friends Blog -->
                
                <div style="margin-top: 20px;">
                    <h5 class="text-center">FRIENDS</h5>
                    <ul class="list-inline text-center">
                        
                        <li><a href="https://hellozhaozheng.github.io/">ZZ</a></li>
                        
                        <li><a href="http://www.jianshu.com/u/e71990ada2fd">简书·BY</a></li>
                        
                        <li><a href="https://apple.com">Apple</a></li>
                        
                    </ul>
                </div>
                
            </div>
        </div>

    </div>
</article>







<!-- Footer -->
<!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1 text-center">
                <br>
                <ul class="list-inline text-center">
                
                
                    <li>
                        <a target="_blank" href="https://twitter.com/ehazon">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                
                

                
                    <li>
                        <a target="_blank" href="http://weibo.com/1254617623">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-weibo fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                
                    <li>
                        <a target="_blank" href="https://github.com/xinghanzzy">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                

                

                </ul>
                <p class="copyright text-muted">
                    Copyright &copy; Boxiao 2019
                    <br>
                    <span id="busuanzi_container_site_pv" style="font-size: 12px;">PV: <span id="busuanzi_value_site_pv"></span> Times</span>
                    <br>
                    Theme by <a href="https://haojen.github.io/">Haojen Ma</a>
                </p>

            </div>
        </div>
    </div>
</footer>

<!-- jQuery -->
<script src="/js/jquery.min.js"></script>

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js"></script>

<!-- Custom Theme JavaScript -->
<script src="/js/blog.js"></script>

<!-- async load function -->
<script>
    function async(u, c) {
      var d = document, t = 'script',
          o = d.createElement(t),
          s = d.getElementsByTagName(t)[0];
      o.src = u;
      if (c) { o.addEventListener('load', function (e) { c(null, e); }, false); }
      s.parentNode.insertBefore(o, s);
    }
</script>

<!-- jquery.tagcloud.js -->
<script>
    // only load tagcloud.js in tag.html
    if($('#tag_cloud').length !== 0){
        async("http://xinghanzzy.github.io/js/jquery.tagcloud.js",function(){
            $.fn.tagcloud.defaults = {
                //size: {start: 1, end: 1, unit: 'em'},
                color: {start: '#bbbbee', end: '#0085a1'},
            };
            $('#tag_cloud a').tagcloud();
        })
    }
</script>

<!--fastClick.js -->
<script>
    async("//cdn.bootcss.com/fastclick/1.0.6/fastclick.min.js", function(){
        var $nav = document.querySelector("nav");
        if($nav) FastClick.attach($nav);
    })
</script>



<!-- Google Analytics -->



<!-- Baidu Tongji -->


<!-- swiftype -->
<script type="text/javascript">
  (function(w,d,t,u,n,s,e){w['SwiftypeObject']=n;w[n]=w[n]||function(){
  (w[n].q=w[n].q||[]).push(arguments);};s=d.createElement(t);
  e=d.getElementsByTagName(t)[0];s.async=1;s.src=u;e.parentNode.insertBefore(s,e);
  })(window,document,'script','//s.swiftypecdn.com/install/v2/st.js','_st');

  _st('install','','2.0.0');
</script>

<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!--wechat title img-->
<img class="wechat-title-img" src="/images/wangye.jpg">
</body>

</html>
